{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3718afce-92be-45db-aeb1-29efa7836e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"../science.mplstyle\")\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "from helpers.data_transforms import inverse_transform, clean_data\n",
    "from helpers.BDT import *\n",
    "from helpers.physics_functions import *\n",
    "from helpers.plotting import hist_all_features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d4bcf3d-fe26-41b3-a5be-c1c71d5b27ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import torch\n",
    "# selecting appropriate device\n",
    "CUDA = torch.cuda.is_available()\n",
    "print(\"cuda available:\", CUDA)\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e897e371-9d9e-41bf-87f5-24b1ff270260",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"workflow.yaml\", \"r\") as file:\n",
    "    workflow = yaml.safe_load(file)\n",
    "    \n",
    "    \n",
    "config_id = \"CATHODE_8\"\n",
    "\n",
    "configs_path = f\"configs/{config_id}.yml\"\n",
    "with open(configs_path, \"r\") as file:\n",
    "    flow_configs = yaml.safe_load(file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e25a357-130c-41b4-83ab-17c6e360220a",
   "metadata": {},
   "source": [
    "Train classifier to discriminate train samples from train data in the SR ONLY\n",
    "By default, the test set data type is the same as `train_data_id`. There are 2 alternative test sets:\n",
    "- If the train data is opp sign, one alt test set will be same sign (and vice versa)\n",
    "- The test set used to construct the ROC curve, which is evaluated on a high-stats dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a1861-a976-4943-86f7-b579ada0f819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in the data\n",
    "\n",
    "bands = [\"SBL\", \"SR\", \"SBH\"]\n",
    "data_dict = {}\n",
    "\n",
    "working_dir = \"/global/cfs/cdirs/m3246/rmastand/dimuonAD/projects/logit_08_22/\"\n",
    "#working_dir = \"/global/u1/r/rmastand/dimuonAD/projects/powerscaler_0813/\"\n",
    "\n",
    "\n",
    "flow_id = \"double_1\"\n",
    "particle_type = \"upsilon\"\n",
    "project_id = \"lowmass\"\n",
    "\n",
    "\n",
    "train_samples_id = \"_samesign\"\n",
    "train_data_id = train_samples_id\n",
    "if train_data_id == \"\": \n",
    "    alt_test_data_id = \"_samesign\"\n",
    "    train_data_id_title = \"_oppsign\"\n",
    "elif train_data_id == \"_samesign\": \n",
    "    alt_test_data_id = \"\"\n",
    "    train_data_id_title = \"_samesign\"\n",
    "\n",
    "ROC_test_data_id = \"\"\n",
    "\n",
    "with open(f\"{working_dir}/models/{project_id}_{particle_type}{train_samples_id}_nojet/{flow_id}/{config_id}/flow_samples\", \"rb\") as infile: \n",
    "    train_samples_dict = pickle.load(infile)\n",
    "\n",
    "# we actually want the \"test band\" here -- train is just for flow\n",
    "with open(f\"{working_dir}/processed_data/{project_id}_{particle_type}{train_data_id}_nojet_test_band_data\", \"rb\") as infile: \n",
    "    train_data_dict = pickle.load(infile)\n",
    "\n",
    "with open(f\"{working_dir}/processed_data/{project_id}_{particle_type}{alt_test_data_id}_nojet_test_band_data\", \"rb\") as infile: \n",
    "    alt_test_data_dict = pickle.load(infile)\n",
    "\n",
    "    \n",
    "# ROC set is evaluated on the FULL high stats dataset\n",
    "with open(f\"{working_dir}/processed_data/{project_id}_{particle_type}{ROC_test_data_id}_nojet_train_band_data\", \"rb\") as infile: \n",
    "    ROC_test_data_dict = pickle.load(infile)\n",
    "\n",
    "print(f\"Loading classifier train samples from {project_id}_{particle_type}{train_samples_id}\")\n",
    "print(f\"Loading classifier train data from {project_id}_{particle_type}{train_data_id}\")\n",
    "print(f\"Loading alternative test data from {project_id}_{particle_type}{alt_test_data_id}\")\n",
    "print(f\"Loading ROC test data from {project_id}_{particle_type}{ROC_test_data_id}\")\n",
    "print()\n",
    "\n",
    "with open(f\"{working_dir}/models/{project_id}_{particle_type}{train_samples_id}_nojet/{flow_id}/{config_id}/configs.txt\", \"rb\") as infile: \n",
    "    configs = infile.readlines()[0].decode(\"utf-8\")\n",
    "    print(configs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45250b69-4fde-44d6-8ce9-5cf7cdf4b676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assemble the test sets -- consists of both SB and SR\n",
    "\n",
    "feature_set = ['dimu_pt', 'dimu_mass']\n",
    "\n",
    "\n",
    "# test set events: not used during flow training\n",
    "num_events_test_SR = train_data_dict[\"SR\"][\"s_inj_data\"][\"dimu_mass\"].shape[0]\n",
    "test_events_SR = np.empty((num_events_test_SR, len(feature_set)))\n",
    "num_events_test_SB = train_data_dict[\"SBH\"][\"s_inj_data\"][\"dimu_mass\"].shape[0]+train_data_dict[\"SBL\"][\"s_inj_data\"][\"dimu_mass\"].shape[0]\n",
    "test_events_SB = np.empty((num_events_test_SB, len(feature_set)))\n",
    "\n",
    "\n",
    "# alt test set events\n",
    "num_events_alt_test = alt_test_data_dict[\"SR\"][\"s_inj_data\"][\"dimu_mass\"].shape[0]+alt_test_data_dict[\"SBH\"][\"s_inj_data\"][\"dimu_mass\"].shape[0]+alt_test_data_dict[\"SBL\"][\"s_inj_data\"][\"dimu_mass\"].shape[0]\n",
    "alt_test_events = np.empty((num_events_alt_test, len(feature_set)))\n",
    "\n",
    "# ROC test set events\n",
    "num_events_ROC = ROC_test_data_dict[\"SR\"][\"s_inj_data\"][\"dimu_mass\"].shape[0]+ROC_test_data_dict[\"SBH\"][\"s_inj_data\"][\"dimu_mass\"].shape[0]+ROC_test_data_dict[\"SBL\"][\"s_inj_data\"][\"dimu_mass\"].shape[0]\n",
    "ROC_test_events = np.empty((num_events_ROC, len(feature_set)))\n",
    "\n",
    "for i, feat in enumerate(feature_set):\n",
    "    \n",
    "    # default test set\n",
    "    test_events_SR[:,i] = train_data_dict[\"SR\"][\"s_inj_data\"][feat].reshape(-1,)\n",
    "    loc_arr = np.concatenate((train_data_dict[\"SBL\"][\"s_inj_data\"][feat], train_data_dict[\"SBH\"][\"s_inj_data\"][feat]))\n",
    "    test_events_SB[:,i] = loc_arr.reshape(-1,)\n",
    "\n",
    "    # alt test set\n",
    "    loc_arr = np.concatenate((alt_test_data_dict[\"SR\"][\"s_inj_data\"][feat], alt_test_data_dict[\"SBL\"][\"s_inj_data\"][feat], alt_test_data_dict[\"SBH\"][\"s_inj_data\"][feat]))\n",
    "    alt_test_events[:,i] = loc_arr.reshape(-1,)\n",
    "    \n",
    "    # ROC test set\n",
    "    loc_arr = np.concatenate((ROC_test_data_dict[\"SR\"][\"s_inj_data\"][feat], ROC_test_data_dict[\"SBL\"][\"s_inj_data\"][feat], ROC_test_data_dict[\"SBH\"][\"s_inj_data\"][feat]))\n",
    "    ROC_test_events[:,i] = loc_arr.reshape(-1,)\n",
    "\n",
    "\n",
    "    \n",
    "print(f\"Total number of default test events: {test_events_SR.shape[0]+test_events_SB.shape[0]}. {test_events_SR.shape[0]} in SR, {test_events_SB.shape[0]} in SB.\")\n",
    "print(f\"Total number of alt test events: {num_events_alt_test}\")\n",
    "print(f\"Total number of ROC test events: {num_events_ROC}\")\n",
    "\n",
    "\n",
    "SR_min_rescaled = np.min(test_events_SR[:,-1])\n",
    "SR_max_rescaled = np.max(test_events_SR[:,-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95116f02-adb1-424f-802c-ddae36c6f830",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot things in the SR\n",
    "\n",
    "print(f\"Training in SR with id: {train_samples_id}.\")\n",
    "hist_all_features_array([train_samples_dict[\"SR\"], train_samples_dict[\"SR_samples\"]], [\"SR data\", \"SR samples\"], \n",
    "                        feature_set, plot_bound = 3, yscale_log = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a798f-269c-426c-8763-3cdb2b48b6c1",
   "metadata": {},
   "source": [
    "## Train the BDTs\n",
    "\n",
    "Train to discriminate (flow samples in SR) from (test data in SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca89e55-dff5-49ef-9883-f6f89e0b9dee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# BDT HYPERPARAMETERS \n",
    "\n",
    "n_estimators = 300 # number of boosting stages\n",
    "max_depth = 5 # max depth of individual regression estimators; related to complexity\n",
    "learning_rate = 0.1\n",
    "subsample = 0.7 # fraction of samples to be used for fitting the individual base learners\n",
    "early_stopping_rounds = 10 # stop training BDT is validation loss doesn't improve after this many rounds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a053c6c7-eff1-4a28-b9d0-184cf09ca5f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_BDT_bump_hunt(flow_samples_SR, data_samples_SR, data_samples_SB, num_folds, num_to_ensemble, \n",
    "                      alt_test_sets_data={}, visualize=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Classifier is trained only on SR data, to distinguish flow SR samples from SR data\n",
    "    \n",
    "    Classifier is evaluated on test data from SR AND SB\n",
    "    \n",
    "    Note that alt test sets are NOT split into folds, since we aren't training on them. We do get diff scores for each fold\n",
    "    \"\"\"\n",
    "    \n",
    "    test_data_splits  = {i:0 for i in range(num_folds)}\n",
    "    scores_splits = {i:0 for i in range(num_folds)}\n",
    "    \n",
    "    # split the alternative test sets\n",
    "    alt_scores_splits = {}\n",
    "    alt_data_splits = {}\n",
    "    for alt_id in alt_test_sets_data.keys():\n",
    "        alt_scores_splits[alt_id] = {i:0 for i in range(num_folds)}\n",
    "        # generate a nfold split for the alt test data\n",
    "        loc_alt_data_split = np.array_split(shuffle(alt_test_sets_data[alt_id]), num_folds) \n",
    "        alt_data_splits[alt_id] = {i:loc_alt_data_split[i] for i in range(num_folds)}\n",
    "        \n",
    "    # shuffle anything with SB data to mix the low and high masses before splitting \n",
    "    flow_samples_SR = shuffle(flow_samples_SR)\n",
    "    data_samples_SR = shuffle(data_samples_SR)\n",
    "    data_samples_SB = shuffle(data_samples_SB)\n",
    "    \n",
    "    flow_SR_splits = np.array_split(flow_samples_SR, num_folds)\n",
    "    data_SR_splits = np.array_split(data_samples_SR, num_folds)\n",
    "    data_SB_splits = np.array_split(data_samples_SB, num_folds) \n",
    "    \n",
    "    for i_fold in range(num_folds):\n",
    "            \n",
    "        print(f\"Fold {i_fold}:\")\n",
    "        \n",
    "        \"\"\"\n",
    "        ASSEMBLE THE TRAIN / VAL / TEST DATA\n",
    "        \"\"\"\n",
    "        \n",
    "        # Assemble the train / test data\n",
    "        training_data, training_labels = [], []\n",
    "        validation_data, validation_labels = [], []\n",
    "        testing_data = []\n",
    "\n",
    "        for ii in range(num_folds):\n",
    "            \n",
    "            # test set comprised of SR and SB data\n",
    "            if ii == i_fold:\n",
    "                testing_data.append(data_SR_splits[ii])\n",
    "                testing_data.append(data_SB_splits[ii])\n",
    "                \n",
    "            # validation set: flow SR samples, data SR sampkes\n",
    "            elif ((ii+1)%num_folds) == i_fold:\n",
    "                validation_data.append(flow_SR_splits[ii])\n",
    "                validation_labels.append(np.zeros((flow_SR_splits[ii].shape[0],1)))\n",
    "                validation_data.append(data_SR_splits[ii])\n",
    "                validation_labels.append(np.ones((data_SR_splits[ii].shape[0],1)))\n",
    "                \n",
    "            else:\n",
    "                training_data.append(flow_SR_splits[ii])\n",
    "                training_labels.append(np.zeros((flow_SR_splits[ii].shape[0],1)))\n",
    "                training_data.append(data_SR_splits[ii])\n",
    "                training_labels.append(np.ones((data_SR_splits[ii].shape[0],1)))\n",
    "                \n",
    "        X_train_fold = np.concatenate(training_data)\n",
    "        Y_train_fold = np.concatenate(training_labels)\n",
    "        X_val_fold = np.concatenate(validation_data)\n",
    "        Y_val_fold = np.concatenate(validation_labels)\n",
    "        \n",
    "        X_test_fold = np.concatenate(testing_data)\n",
    "        \n",
    "        # record the local fold data\n",
    "        test_data_splits[i_fold] = X_test_fold\n",
    "     \n",
    "        \"\"\"\n",
    "        SORT THE WEIGHTS OUT\n",
    "        \"\"\"\n",
    "        \n",
    "        # First do the weights for the regular BC (non-decorr)\n",
    "        class_weight = {0: 1, 1: sum(Y_train_fold==0)[0]/sum(Y_train_fold==1)[0]}\n",
    "        class_weights_train = class_weight[0]*(1.0-Y_train_fold)+class_weight[1]*Y_train_fold\n",
    "        class_weights_val = class_weight[0]*(1.0-Y_val_fold)+class_weight[1]*Y_val_fold\n",
    "        \n",
    "        \"\"\"\n",
    "        COMBINE W/ DECORRELATED TRAINING\n",
    "        \"\"\"\n",
    "        # we only want to train on the non-mass features\n",
    "\n",
    "        X_train_fold = X_train_fold[:,:-1]\n",
    "        X_val_fold = X_val_fold[:,:-1]\n",
    "\n",
    "        w_train_fold = class_weights_train\n",
    "        w_val_fold = class_weights_val\n",
    "\n",
    "        # shuffle for good measure\n",
    "        X_train_fold, Y_train_fold, w_train_fold = shuffle(X_train_fold, Y_train_fold, w_train_fold)\n",
    "        X_val_fold, Y_val_fold, w_val_fold = shuffle(X_val_fold, Y_val_fold, w_val_fold)\n",
    "        \n",
    "        X_test_fold = X_test_fold[:,:-1]\n",
    "        \n",
    "        \n",
    "        print(f\"X train shape: {X_train_fold.shape}, Y train shape: {Y_train_fold.shape}, w train shape: {w_train_fold.shape}.\")\n",
    "        print(f\"X val shape: {X_val_fold.shape}, Y val shape: {Y_val_fold.shape}, w val shape: {w_val_fold.shape}.\")\n",
    "        print(f\"X test shape: {X_test_fold.shape}.\" )\n",
    "        \n",
    "        \"\"\"\n",
    "        INITIALIZE SCORE OBJECTS\n",
    "        \"\"\"\n",
    "        \n",
    "        scores_fold = np.empty((X_test_fold.shape[0], num_to_ensemble))\n",
    "        alt_scores_fold = {}\n",
    "        for alt_id in alt_test_sets_data.keys():\n",
    "            alt_scores_fold[alt_id] = np.empty((alt_data_splits[alt_id][i_fold].shape[0], num_to_ensemble))\n",
    "            \n",
    "   \n",
    "        \"\"\"\n",
    "        TRAIN ENSEMBLE OF TREES\n",
    "        \"\"\"\n",
    "    \n",
    "        if visualize:\n",
    "            plt.figure()\n",
    "        \n",
    "        for i_tree in range(num_to_ensemble):\n",
    "            \n",
    "            print(\"   Network number:\", i_tree)\n",
    "            random_seed = i_fold*num_to_ensemble + i_tree + 1\n",
    "                \n",
    "                \n",
    "            eval_set = [(X_train_fold, Y_train_fold), (X_val_fold, Y_val_fold)]\n",
    "            \n",
    "\n",
    "            bst_i = xgb.XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, \n",
    "                              subsample=subsample,  early_stopping_rounds=early_stopping_rounds,\n",
    "                              objective='binary:logistic', \n",
    "                                      random_state = random_seed, eval_metric=\"logloss\")\n",
    "\n",
    "            bst_i.fit(X_train_fold, Y_train_fold, sample_weight=w_train_fold, \n",
    "                      eval_set=eval_set, sample_weight_eval_set = [w_train_fold, w_val_fold],\n",
    "                      verbose=False)\n",
    "            results_f = bst_i.evals_result()\n",
    "            losses = results_f[\"validation_0\"][\"logloss\"]\n",
    "            losses_val = results_f[\"validation_1\"][\"logloss\"]\n",
    "            best_epoch = bst_i.best_iteration\n",
    "\n",
    "\n",
    "            # get scores\n",
    "            scores_fold[:,i_tree] = bst_i.predict_proba(X_test_fold, iteration_range=(0,bst_i.best_iteration))[:,1]\n",
    "            for alt_id in alt_test_sets_data.keys():\n",
    "                alt_scores_fold[alt_id][:,i_tree] = bst_i.predict_proba(alt_data_splits[alt_id][i_fold][:,:-1], iteration_range=(0,bst_i.best_iteration))[:,1]\n",
    "\n",
    "\n",
    "            \n",
    "            if visualize:\n",
    "                \n",
    "                plt.plot(losses, label = f\"{i_tree}\", color = f\"C{i_tree}\")\n",
    "                plt.plot(losses_val, color = f\"C{i_tree}\", linestyle = \"dashed\")\n",
    "                plt.xlabel(\"Epochs\")\n",
    "                plt.ylabel(\"Loss\")\n",
    "                plt.axvline(best_epoch, color = f\"C{i_tree}\")\n",
    "                plt.title(f\"Fold {i_fold}\")\n",
    "        \n",
    "        if visualize:\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "            \n",
    "            \n",
    "        \"\"\"\n",
    "        AVERAGE OVER ENSEMBLE\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "            \n",
    "        #scores_splits[i_fold] = np.mean(scores_fold, axis = 1)\n",
    "        scores_splits[i_fold] = scores_fold\n",
    "        for alt_id in alt_test_sets_data.keys():\n",
    "            #alt_scores_splits[alt_id][i_fold] = np.mean(alt_scores_fold[alt_id], axis = 1)\n",
    "            alt_scores_splits[alt_id][i_fold] = alt_scores_fold[alt_id]\n",
    "            \n",
    "        plt.figure()\n",
    "        plt.hist2d(test_data_splits[i_fold][:,-1], np.mean(scores_fold, axis = 1), bins = 40, cmap = \"hot\", density = True)\n",
    "        plt.xlabel(\"M (rescaled)\")\n",
    "        plt.ylabel(\"score\")\n",
    "        #plt.axvline(SR_min_rescaled, color = \"red\")\n",
    "        #plt.axvline(SR_max_rescaled, color = \"red\")\n",
    "        plt.colorbar()\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.hist2d(alt_data_splits[\"FPR_validation\"][i_fold][:,-1], np.mean(alt_scores_splits[\"FPR_validation\"][i_fold], axis = 1), bins = 40, cmap = \"hot\", density = True)\n",
    "        plt.xlabel(\"M (rescaled)\")\n",
    "        plt.ylabel(\"score\")\n",
    "        plt.axvline(SR_min_rescaled, color = \"red\")\n",
    "        plt.axvline(SR_max_rescaled, color = \"red\")\n",
    "        plt.title(\"FPR_validation\")\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.hist2d(alt_data_splits[\"samesign\"][i_fold][:,-1], np.mean(alt_scores_splits[\"samesign\"][i_fold], axis = 1), bins = 40, cmap = \"hot\", density = True)\n",
    "        plt.xlabel(\"M (rescaled)\")\n",
    "        plt.ylabel(\"score\")\n",
    "        #plt.axvline(SR_min_rescaled, color = \"red\")\n",
    "        #plt.axvline(SR_max_rescaled, color = \"red\")\n",
    "        plt.title(\"samesign\")\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        print()\n",
    "        \"\"\"\n",
    "        \n",
    "    return test_data_splits, scores_splits, alt_data_splits, alt_scores_splits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1c179-eb9f-476a-b19d-3e5d5fd9d597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train a classifier to discriminate SR samples from SR data\n",
    "\n",
    "n_folds = 5\n",
    "n_to_ensemble = 20\n",
    "\n",
    "alt_test_sets_data = {\"FPR_validation\":train_samples_dict[\"SR_samples_validation\"][:,:-1],\n",
    "                      \"alt\":alt_test_events,\n",
    "                      \"ROC\":ROC_test_events}\n",
    "\n",
    "\n",
    "\n",
    "test_data_splits, scores_splits, alt_data_splits, alt_scores_splits = run_BDT_bump_hunt(clean_data(train_samples_dict[\"SR_samples\"][:,:-1]), \n",
    "                                        clean_data(test_events_SR), clean_data(test_events_SB), n_folds, n_to_ensemble, \n",
    "                                            alt_test_sets_data=alt_test_sets_data,\n",
    "                                                                     visualize=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07104b01-1593-41b0-995f-57f766c02571",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeddcb87-fdfb-4663-a443-b8a4d84edac4",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### True bump hunt\n",
    "\n",
    "- Define a polynomial form for the background fit\n",
    "- For each cut on the classifier score:\n",
    "  - Fit the background on the cut SB data\n",
    "  - Estimate N_bkg\n",
    "  - Calculate S/B, S/sqrt(B)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59179b8-f8ae-482c-af38-4976b0682e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{working_dir}/processed_data/mass_scaler_{particle_type}\", \"rb\") as ifile:\n",
    "    scaler = pickle.load(ifile)\n",
    "    \n",
    "fpr_thresholds = [1, 0.25, 0.1, 0.05, 0.01]\n",
    "#score_cutoffs = {i:{threshold:np.zeros((n_to_ensemble,1)) for threshold in fpr_thresholds} for i in range(n_folds)}\n",
    "score_cutoffs = {i:{threshold:0 for threshold in fpr_thresholds} for i in range(n_folds)}\n",
    "\n",
    "\n",
    "for i_fold in range(n_folds):\n",
    "    for threshold in fpr_thresholds:\n",
    "        \n",
    "        #for i_classifier in range(n_to_ensemble): # may want to do ensembling later?\n",
    "        \n",
    "            loc_scores_sorted = np.sort(1.0-np.mean(alt_scores_splits[\"FPR_validation\"][i_fold], axis = 1))\n",
    "            loc_score_cutoff = 1-loc_scores_sorted[min(int(threshold*len(loc_scores_sorted)),len(loc_scores_sorted)-1)]\n",
    "            \n",
    "            score_cutoffs[i_fold][threshold] = loc_score_cutoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1c778-a9d2-44de-ae0c-2e345532de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SB_left = float(workflow[particle_type][\"SB_left\"])\n",
    "SR_left = float(workflow[particle_type][\"SR_left\"])\n",
    "SR_right = float(workflow[particle_type][\"SR_right\"])\n",
    "SB_right = float(workflow[particle_type][\"SB_right\"])\n",
    "    \n",
    "TPRs, FPRs = [], []\n",
    "\n",
    "remove_edge = True\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "x = np.linspace(SB_left, SB_right, 100)\n",
    "\n",
    "# get bins, bin centers\n",
    "plot_bins_all, plot_bins_SR, plot_bins_left, plot_bins_right, plot_centers_all, plot_centers_SR, plot_centers_SB = get_bins(SR_left, SR_right, SB_left, SB_right, remove_edge)\n",
    "\n",
    "\n",
    "fit_type = \"cubic\"\n",
    "if fit_type == \"cubic\": fit_function = bkg_fit_cubic\n",
    "elif fit_type == \"quintic\": fit_function = bkg_fit_quintic\n",
    "elif fit_type == \"ratio\": fit_function = bkg_fit_ratio\n",
    "\n",
    "\n",
    "plt.figure(figsize = (12, 9))\n",
    "for t, threshold in enumerate(fpr_thresholds):\n",
    "    \n",
    "    # corrections to SR / SB efficiencies\n",
    "    filtered_masses = []\n",
    "    \n",
    "    for i_fold in range(n_folds):\n",
    "        \n",
    "        \n",
    "        loc_true_masses = scaler.inverse_transform(np.array(test_data_splits[i_fold][:,-1]).reshape(-1,1))\n",
    "        loc_scores = np.mean(scores_splits[i_fold], axis = 1)\n",
    "        loc_filtered_masses, loc_SBL_eff, loc_SBH_eff, loc_SR_eff = select_top_events_fold(loc_true_masses, loc_scores, score_cutoffs[i_fold][threshold],\n",
    "                                                                                           plot_bins_left, plot_bins_right, plot_bins_SR)\n",
    "        \n",
    "\n",
    "        filtered_masses.append(loc_filtered_masses)\n",
    "\n",
    "        \n",
    "\n",
    "    filtered_masses = np.concatenate(filtered_masses)\n",
    "\n",
    "    \n",
    "    # get the fit function to SB background\n",
    "    popt, pcov, chi2, y_vals, n_dof = curve_fit_m_inv(filtered_masses, fit_type, SR_left, SR_right, plot_bins_left, plot_bins_right, plot_centers_all,)\n",
    "    #print(\"chi2/dof:\", chi2/n_dof)\n",
    "    # plot the fit function\n",
    "    plt.plot(plot_centers_all, fit_function(plot_centers_all, *popt), lw = 2, linestyle = \"dashed\", color = f\"C{t}\")    \n",
    "    \n",
    "    # calculate significance of bump\n",
    "    num_S_expected_in_SR, num_B_expected_in_SR = calc_significance(filtered_masses, fit_function, plot_bins_SR, SR_left, SR_right, popt)\n",
    "    \n",
    "    if threshold == 1:\n",
    "        S_total, B_total = num_S_expected_in_SR, num_B_expected_in_SR\n",
    "    \n",
    "    TPRs.append(num_S_expected_in_SR/S_total)\n",
    "    FPRs.append(num_B_expected_in_SR/B_total)\n",
    "        \n",
    "\n",
    "    y_err = get_errors_bkg_fit_ratio(popt, pcov, plot_centers_SR, fit_type)\n",
    "    B_error = np.sum(y_err)\n",
    "\n",
    "    label_string = str(round(100*threshold, 2))+\"% FPR: $S/B$: \"+str(round(num_S_expected_in_SR/num_B_expected_in_SR,2))+\", $S/\\sqrt{B}$: \"+str(round(num_S_expected_in_SR/np.sqrt(num_B_expected_in_SR+B_error**2),2))\n",
    "\n",
    "    plt.hist(filtered_masses, bins = plot_bins_all, lw = 3, histtype = \"step\", color = f\"C{t}\",label = label_string)\n",
    "    plt.scatter(plot_centers_SB, y_vals, color = f\"C{t}\")\n",
    "    \n",
    "    \n",
    "\n",
    "print() \n",
    "plt.legend(loc = (1, 0), fontsize = 24)\n",
    "\n",
    "\n",
    "plt.axvline(SR_left, color= \"k\", lw = 3, zorder = -10)\n",
    "plt.axvline(SR_right, color= \"k\", lw = 3, zorder = -10)\n",
    "\n",
    "plt.xlabel(\"$M_{\\mu\\mu}$ [GeV]\", fontsize = 24)\n",
    "plt.ylabel(\"Counts\", fontsize = 24)\n",
    "\n",
    "\n",
    "plt.title(f\"{particle_type}{train_data_id_title} (trained on {train_data_id_title})\\n\"+\"  \".join(feature_set[:-1])+\"\\n\", fontsize = 24)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f955c9-56f5-4b6e-ab6b-19dedc13bfc1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Repeat for alternative test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839fc158-3cb8-4a29-b276-441e4cf77453",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_type = \"cubic\"\n",
    "if fit_type == \"cubic\": fit_function = bkg_fit_cubic\n",
    "elif fit_type == \"quintic\": fit_function = bkg_fit_quintic\n",
    "elif fit_type == \"ratio\": fit_function = bkg_fit_ratio\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize = (12, 9))\n",
    "for t, threshold in enumerate(fpr_thresholds):\n",
    "    \n",
    "    # corrections to SR / SB efficiencies\n",
    "    filtered_masses = []\n",
    "    \n",
    "    for i_fold in range(n_folds):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        loc_true_masses = scaler.inverse_transform(alt_data_splits[\"alt\"][i_fold][:,-1].reshape(-1,1))\n",
    "        loc_scores = np.mean(alt_scores_splits[\"alt\"][i_fold], axis = 1)\n",
    "        loc_filtered_masses, loc_SBL_eff, loc_SBH_eff, loc_SR_eff = select_top_events_fold(loc_true_masses, loc_scores, score_cutoffs[i_fold][threshold],\n",
    "                                                                                           plot_bins_left, plot_bins_right, plot_bins_SR)\n",
    "        \n",
    "        filtered_masses.append(loc_filtered_masses)\n",
    "\n",
    "        \n",
    "    filtered_masses = np.concatenate(filtered_masses)\n",
    "\n",
    "    \n",
    "    # get the fit function to SB background\n",
    "    popt, pcov, chi2, y_vals, n_dof = curve_fit_m_inv(filtered_masses, fit_type, SR_left, SR_right, plot_bins_left, plot_bins_right, plot_centers_all)\n",
    "    #print(\"chi2/dof:\", chi2/n_dof)\n",
    "    # plot the fit function\n",
    "    plt.plot(plot_centers_all, fit_function(plot_centers_all, *popt), lw = 2, linestyle = \"dashed\", color = f\"C{t}\")    \n",
    "    \n",
    "    # calculate significance of bump\n",
    "    num_S_expected_in_SR, num_B_expected_in_SR = calc_significance(filtered_masses, fit_function, plot_bins_SR, SR_left, SR_right, popt)\n",
    "    #print(num_S_expected_in_SR, num_B_expected_in_SR)\n",
    "\n",
    "    y_err = get_errors_bkg_fit_ratio(popt, pcov, plot_centers_SR, fit_type)\n",
    "    B_error = np.sum(y_err)\n",
    "\n",
    "    label_string = str(round(100*threshold, 2))+\"% FPR: $S/B$: \"+str(round(num_S_expected_in_SR/num_B_expected_in_SR,2))+\", $S/\\sqrt{B}$: \"+str(round(num_S_expected_in_SR/np.sqrt(num_B_expected_in_SR+B_error**2),2))\n",
    "\n",
    "    plt.hist(filtered_masses, bins = plot_bins_all, lw = 3, histtype = \"step\", color = f\"C{t}\",label = label_string)\n",
    "    plt.scatter(plot_centers_SB, y_vals, color = f\"C{t}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#plt.title(\",  \".join(feature_set[:-1])+\"\\n\", fontsize = 18) \n",
    "plt.legend(loc = (1, 0), fontsize = 24)\n",
    "#plt.yscale(\"log\")\n",
    "\n",
    "plt.axvline(SR_left, color= \"k\", lw = 3, zorder = -10)\n",
    "plt.axvline(SR_right, color= \"k\", lw = 3, zorder = -10)\n",
    "\n",
    "plt.xlabel(\"$M_{\\mu\\mu}$ [GeV]\", fontsize = 24)\n",
    "plt.ylabel(\"Counts\", fontsize = 24)\n",
    "\n",
    "\n",
    "\n",
    "plt.title(f\"{particle_type}{alt_test_data_id} (trained on {train_data_id_title})\\n\"+\"  \".join(feature_set[:-1])+\"\\n\", fontsize = 24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba590ed-c5b3-4a1c-a89d-9e63c3a61ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a4d92-7af8-4847-87d1-d39b52faef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "SB_left = float(workflow[particle_type][\"SB_left\"])\n",
    "SR_left = float(workflow[particle_type][\"SR_left\"])\n",
    "SR_right = float(workflow[particle_type][\"SR_right\"])\n",
    "SB_right = float(workflow[particle_type][\"SB_right\"])\n",
    "    \n",
    "TPRs, FPRs = [], []\n",
    "\n",
    "remove_edge = True\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "x = np.linspace(SB_left, SB_right, 100)\n",
    "\n",
    "# get bins, bin centers\n",
    "plot_bins_all, plot_bins_SR, plot_bins_left, plot_bins_right, plot_centers_all, plot_centers_SR, plot_centers_SB = get_bins(SR_left, SR_right, SB_left, SB_right, remove_edge)\n",
    "\n",
    "\n",
    "fit_type = \"cubic\"\n",
    "if fit_type == \"cubic\": fit_function = bkg_fit_cubic\n",
    "elif fit_type == \"quintic\": fit_function = bkg_fit_quintic\n",
    "elif fit_type == \"ratio\": fit_function = bkg_fit_ratio\n",
    "\n",
    "\n",
    "plt.figure(figsize = (12, 9))\n",
    "for t, threshold in enumerate(fpr_thresholds):\n",
    "    \n",
    "    # corrections to SR / SB efficiencies\n",
    "    filtered_masses = []\n",
    "    \n",
    "    for i_fold in range(n_folds):\n",
    "        \n",
    "        \n",
    "        loc_true_masses = scaler.inverse_transform(np.array(alt_data_splits[\"ROC\"][i_fold][:,-1]).reshape(-1,1))\n",
    "        loc_scores = np.mean(alt_scores_splits[\"ROC\"][i_fold], axis = 1)\n",
    "        loc_filtered_masses, loc_SBL_eff, loc_SBH_eff, loc_SR_eff = select_top_events_fold(loc_true_masses, loc_scores, score_cutoffs[i_fold][threshold],\n",
    "                                                                                           plot_bins_left, plot_bins_right, plot_bins_SR)\n",
    "        \n",
    "\n",
    "        filtered_masses.append(loc_filtered_masses)\n",
    "\n",
    "        \n",
    "\n",
    "    filtered_masses = np.concatenate(filtered_masses)\n",
    "\n",
    "    \n",
    "    # get the fit function to SB background\n",
    "    popt, pcov, chi2, y_vals, n_dof = curve_fit_m_inv(filtered_masses, fit_type, SR_left, SR_right, plot_bins_left, plot_bins_right, plot_centers_all,)\n",
    "    #print(\"chi2/dof:\", chi2/n_dof)\n",
    "    # plot the fit function\n",
    "    plt.plot(plot_centers_all, fit_function(plot_centers_all, *popt), lw = 2, linestyle = \"dashed\", color = f\"C{t}\")    \n",
    "    \n",
    "    # calculate significance of bump\n",
    "    num_S_expected_in_SR, num_B_expected_in_SR = calc_significance(filtered_masses, fit_function, plot_bins_SR, SR_left, SR_right, popt)\n",
    "\n",
    "\n",
    "    y_err = get_errors_bkg_fit_ratio(popt, pcov, plot_centers_SR, fit_type)\n",
    "    B_error = np.sum(y_err)\n",
    "\n",
    "    label_string = str(round(100*threshold, 2))+\"% FPR: $S/B$: \"+str(round(num_S_expected_in_SR/num_B_expected_in_SR,2))+\", $S/\\sqrt{B}$: \"+str(round(num_S_expected_in_SR/np.sqrt(num_B_expected_in_SR+B_error**2),2))\n",
    "\n",
    "    plt.hist(filtered_masses, bins = plot_bins_all, lw = 3, histtype = \"step\", color = f\"C{t}\",label = label_string)\n",
    "    plt.scatter(plot_centers_SB, y_vals, color = f\"C{t}\")\n",
    "    \n",
    "    \n",
    "\n",
    "print() \n",
    "plt.legend(loc = (1, 0), fontsize = 24)\n",
    "\n",
    "\n",
    "plt.axvline(SR_left, color= \"k\", lw = 3, zorder = -10)\n",
    "plt.axvline(SR_right, color= \"k\", lw = 3, zorder = -10)\n",
    "\n",
    "plt.xlabel(\"$M_{\\mu\\mu}$ [GeV]\", fontsize = 24)\n",
    "plt.ylabel(\"Counts\", fontsize = 24)\n",
    "plt.title(f\"{particle_type} {ROC_test_data_id} (trained on {train_data_id_title})\\n\"+\"  \".join(feature_set[:-1])+\"\\n\", fontsize = 24)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b67ff2-d8bf-4b2d-921e-b981367be7ef",
   "metadata": {},
   "source": [
    "## ROC curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630f3622-44d8-49a1-aae1-b60a2ca1f0e6",
   "metadata": {},
   "source": [
    "### True classifier uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd23236-c8d7-4340-b123-0246c5c99e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "TPRs, FPRs = [], []\n",
    "\n",
    "remove_edge = True\n",
    "\n",
    "\n",
    "\n",
    "fit_type = \"cubic\"\n",
    "if fit_type == \"cubic\": fit_function = bkg_fit_cubic\n",
    "elif fit_type == \"quintic\": fit_function = bkg_fit_quintic\n",
    "elif fit_type == \"ratio\": fit_function = bkg_fit_ratio\n",
    "\n",
    "\n",
    "\n",
    "fpr_thresholds = np.logspace(0, -3, 50)\n",
    "#fpr_thresholds = np.linspace(1, 0 , 50)\n",
    "\n",
    "\n",
    "\n",
    "# first determine score cutoffs\n",
    "score_cutoffs = {i:{threshold:np.zeros((n_to_ensemble,1)) for threshold in fpr_thresholds} for i in range(n_folds)}\n",
    "\n",
    "for t, threshold in enumerate(fpr_thresholds):\n",
    "    for i_fold in range(n_folds):\n",
    "        for i_classifier in range(n_to_ensemble):\n",
    "            loc_scores_sorted = np.sort(1.0-alt_scores_splits[\"FPR_validation\"][i_fold][:,i_classifier])\n",
    "            score_cutoffs[i_fold][threshold][i_classifier] = 1-loc_scores_sorted[min(int(threshold*len(loc_scores_sorted)),len(loc_scores_sorted)-1)]\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "S_yield, B_yield = np.empty((fpr_thresholds.shape[0], n_to_ensemble)), np.empty((fpr_thresholds.shape[0], n_to_ensemble))\n",
    "\n",
    "for bs in range(n_to_ensemble):\n",
    "    \n",
    "    print(f\"On classifier index {bs}...\")\n",
    "\n",
    "    for t, threshold in enumerate(fpr_thresholds):\n",
    "    \n",
    "        filtered_masses_bs = []\n",
    "    \n",
    "        for i_fold in range(n_folds):\n",
    "\n",
    "            loc_true_masses_bs = scaler.inverse_transform(np.array(alt_data_splits[\"ROC\"][i_fold][:,-1]).reshape(-1,1))\n",
    "            loc_scores_bs = alt_scores_splits[\"ROC\"][i_fold][:,bs]\n",
    "            # filter top event based on score cutoff\n",
    "            loc_filtered_masses_bs, _, _, _ = select_top_events_fold(loc_true_masses_bs, loc_scores_bs, score_cutoffs[i_fold][threshold][bs], plot_bins_left, plot_bins_right, plot_bins_SR)\n",
    "            filtered_masses_bs.append(loc_filtered_masses_bs)\n",
    "\n",
    "        filtered_masses_bs = np.concatenate(filtered_masses_bs)\n",
    "\n",
    "        # get the fit function to SB background\n",
    "        popt, pcov, chi2, y_vals, n_dof = curve_fit_m_inv(filtered_masses_bs, fit_type, SR_left, SR_right, plot_bins_left, plot_bins_right, plot_centers_all)\n",
    "        num_S_expected_in_SR, num_B_expected_in_SR = calc_significance(filtered_masses_bs, fit_function, plot_bins_SR, SR_left, SR_right, popt)\n",
    "        \n",
    "        S_yield[t, bs] = num_S_expected_in_SR\n",
    "        B_yield[t, bs] = num_B_expected_in_SR\n",
    "\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11771047-cfc7-4da9-af27-ba1380d84bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054ef2a-0291-44c6-a5c5-1028212ae505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# calculate summary stats\n",
    "TPR = S_yield/S_yield[0,:]\n",
    "FPR = B_yield/B_yield[0,:]\n",
    "\n",
    "ROC = 1.0/FPR\n",
    "\n",
    "SIC = TPR/np.sqrt(FPR)\n",
    "\n",
    "\n",
    "def get_median_percentiles(x_array):\n",
    "    \n",
    "    \n",
    "    x_median = np.median(x_array, axis = 1)\n",
    "    x_lower = np.percentile(x_array, 16, axis = 1)\n",
    "    x_upper = np.percentile(x_array, 84, axis = 1)\n",
    "\n",
    "    return x_median, x_lower, x_upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b973efd2-ce46-445a-8408-5ac37c6d82a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR_median, TPR_lower, TPR_upper = get_median_percentiles(TPR)\n",
    "FPR_median, FPR_lower, FPR_upper = get_median_percentiles(FPR)\n",
    "ROC_median, ROC_lower, ROC_upper = get_median_percentiles(ROC)\n",
    "SIC_median, SIC_lower, SIC_upper = get_median_percentiles(SIC)\n",
    "\n",
    "print(f\"Classifier trained on {feature_set[:-1]}\")\n",
    "\n",
    "plt.figure(figsize = (5, 5))\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(FPR_median, TPR_median)\n",
    "plt.fill_between(FPR_median, TPR_lower, TPR_upper, alpha = 0.3 )\n",
    "\n",
    "plt.plot(FPR_median, FPR_median, linestyle = \"dashed\", color = \"grey\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "\n",
    "\n",
    "plt.title(f\"{particle_type} {ROC_test_data_id} (trained on {train_data_id_title})\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10, 4))\n",
    "\n",
    "\n",
    "ax[0].plot(TPR_median, ROC_median)\n",
    "ax[0].fill_between(TPR_median, ROC_lower, ROC_upper, alpha = 0.3 )\n",
    "ax[0].plot(TPR_median, 1.0/TPR_median, linestyle = \"dashed\", color = \"grey\")\n",
    "ax[0].set_xlabel(\"TPR\")\n",
    "ax[0].set_ylabel(\"1/FPR\")\n",
    "ax[0].set_yscale(\"log\")\n",
    "\n",
    "\n",
    "    \n",
    "ax[1].plot(TPR_median, SIC_median)\n",
    "ax[1].fill_between(TPR_median, SIC_lower, SIC_upper, alpha = 0.3 )\n",
    "ax[1].plot(TPR_median, TPR_median/np.sqrt(TPR_median), linestyle = \"dashed\", color = \"grey\")\n",
    "ax[1].set_xlabel(\"TPR\")\n",
    "ax[1].set_ylabel(\"SIC\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef455738-9469-4c74-add4-688bc6ffb7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cb506e-b390-43c4-b35d-407612971b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bee70c1-6da3-47c1-9954-ecf99555caff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67730524-352c-4913-abfa-ee2b8cc22967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc53a3de-fc00-4223-82e5-2d97468a66e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845cc17-052d-487d-8eff-b734f8bf85d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a834f-7be5-4ee8-b2ea-093d6f8e3d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a09783-49ba-4a4e-bdb6-8604a4898177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
