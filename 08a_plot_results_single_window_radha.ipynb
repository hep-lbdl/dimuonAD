{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a61a3-13b2-44af-b9aa-1292b922a7fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "from helpers.physics_functions import bkg_fit_cubic, bkg_fit_septic, bkg_fit_quintic, get_bins, select_top_events_fold, curve_fit_m_inv, calc_significance, get_errors_bkg_fit_ratio\n",
    "from helpers.evaluation import get_median_percentiles\n",
    "from helpers.plotting import newplot, hist_with_outline, hist_with_errors, function_with_band\n",
    "\n",
    "# Try to load LaTeX\n",
    "latex_flag = False\n",
    "\n",
    "np.seterr(divide='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9131a5-14ad-48b6-af76-17d7b0ac5b12",
   "metadata": {},
   "source": [
    "This notebook should be run twice:\n",
    "\n",
    "1. `train_samesign = False` gives the \"standard\" results. i.e. we run the studies on the OS samples\n",
    "2. `train_samesign = True` comes from running the Ml study on the SS samples.\n",
    "\n",
    "**CAUTION**: for the histograms, we are truly showing the significance as $\\frac{S}{\\sqrt{B+{\\sigma_B}^2}}$, i.e. we are accounting for the background error. For the ROC curves, this error is *NOT* being taken into account (it's not clear to me that we want this background error when we are just citing the background yield for the FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6c7ec-8adf-4883-b5c7-0f0a02043d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"workflow.yaml\", \"r\") as file:\n",
    "    workflow = yaml.safe_load(file) \n",
    "\n",
    "# pickles contain all the results from the BDT training\n",
    "pickle_save_dir_prefix = \"/global/cfs/cdirs/m3246/rmastand/dimuonAD/pickles/mix_2\"\n",
    "train_samesign = False\n",
    "\n",
    "working_dir = f\"/global/cfs/cdirs/m3246/rmastand/dimuonAD/projects/logit_08_22/\"\n",
    "\n",
    "# basically hard-coded for the PRL \n",
    "num_pseudoexperiments = 1000 + 1\n",
    "n_folds = 5\n",
    "particle_type = \"upsilon_iso\"\n",
    "particle_id = \"upsilon\"\n",
    "\n",
    "\n",
    "# Needed for Classical Studies\n",
    "jet_id = \"nojet\"\n",
    "config_id = \"CATHODE_8\"\n",
    "project_id = \"lowmass\"\n",
    "flow_id = \"mix_2\"\n",
    "\n",
    "all_TPR = {}\n",
    "all_SIC = {}\n",
    "all_FPR = {}\n",
    "all_ROC = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace8c3d0-83b9-4747-a9c3-fe154828cad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_bins_SR = 16 # 16, 12, 8\n",
    "\n",
    "pseudo_e_to_plot = 0 # this plots the actual data (not a boostrapped version)\n",
    "fit_type = \"cubic\" # \"cubic\", \"quintic\", septic\n",
    "if fit_type == \"cubic\": fit_function = bkg_fit_cubic\n",
    "if fit_type == \"quintic\": fit_function = bkg_fit_quintic\n",
    "if fit_type == \"septic\": fit_function = bkg_fit_septic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e9288-aed1-4383-959a-cbbc5ae3c3d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SB_left = float(workflow[particle_id][\"SB_left\"])\n",
    "SR_left = float(workflow[particle_id][\"SR_left\"])\n",
    "SR_right = float(workflow[particle_id][\"SR_right\"])\n",
    "SB_right = float(workflow[particle_id][\"SB_right\"])\n",
    "\n",
    "# somewhat complicated code to set naming conventions\n",
    "if train_samesign:\n",
    "    train_data_id = \"_samesign\"\n",
    "else:\n",
    "    train_data_id = \"\"\n",
    "\n",
    "# train on opp sign means alt test set is samesign\n",
    "if train_data_id == \"\": \n",
    "    alt_test_data_id = \"_samesign\"\n",
    "    train_data_id_title = \"_oppsign\"\n",
    "elif train_data_id == \"_samesign\": \n",
    "    alt_test_data_id = \"\"\n",
    "    train_data_id_title = \"_samesign\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d54f5",
   "metadata": {},
   "source": [
    "# Load in the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92df6ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in the data corresponding to the train id\n",
    "# we actually want the \"test band\" here -- train is just for flow\n",
    "with open(f\"{working_dir}/processed_data/{project_id}_{particle_type}{train_data_id}_{jet_id}_test_band_data\", \"rb\") as infile: \n",
    "    test_data_dict = pickle.load(infile)\n",
    "\n",
    "with open(f\"{working_dir}/models/{project_id}_{particle_type}{train_data_id}_{jet_id}/{flow_id}/{config_id}/seed1/configs.txt\", \"rb\") as infile: \n",
    "    configs = infile.readlines()[0].decode(\"utf-8\")\n",
    "    \n",
    "    feature_set = [x.strip() for x in configs.split(\"'\")][1::2]\n",
    "\n",
    "print(f\"Feature Set: {feature_set}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820f9de-9e16-4ab1-8e6f-47d1207935f4",
   "metadata": {},
   "source": [
    "## Load in the BDT results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66841771-559c-47ad-aa75-7c7719d4747b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if train_samesign = False, this loads in the OS test data\n",
    "# test data\n",
    "with open(f\"{pickle_save_dir_prefix}_{particle_type}{train_data_id_title}/all_test_data_splits_{fit_type}_{num_bins_SR}_0_501\", \"rb\") as ifile:\n",
    "    loc_all_test_data_splits_0 = pickle.load(ifile)\n",
    "with open(f\"{pickle_save_dir_prefix}_{particle_type}{train_data_id_title}/all_test_data_splits_{fit_type}_{num_bins_SR}_501_1001\", \"rb\") as ifile:\n",
    "    loc_all_test_data_splits_1 = pickle.load(ifile)\n",
    "all_test_data_splits = {**loc_all_test_data_splits_0, **loc_all_test_data_splits_1}\n",
    "print(len(all_test_data_splits.keys())==num_pseudoexperiments)\n",
    "\n",
    "# test scores\n",
    "with open(f\"{pickle_save_dir_prefix}_{particle_type}{train_data_id_title}/all_scores_splits_{fit_type}_{num_bins_SR}_0_501\", \"rb\") as ifile:\n",
    "    loc_all_scores_splits_0 = pickle.load(ifile)\n",
    "with open(f\"{pickle_save_dir_prefix}_{particle_type}{train_data_id_title}/all_scores_splits_{fit_type}_{num_bins_SR}_501_1001\", \"rb\") as ifile:\n",
    "    loc_all_scores_splits_1 = pickle.load(ifile)\n",
    "all_scores_splits = {**loc_all_scores_splits_0, **loc_all_scores_splits_1}\n",
    "print(len(all_scores_splits.keys())==num_pseudoexperiments)\n",
    "\n",
    "# alt data\n",
    "# if train_samesign = False, this loads in the SS test data, OS high-stats data, and OS flow samples\n",
    "# if train_samesign = True, this loads in the OS test data, SS high-stats data, and SS flow samples\n",
    "with open(f\"{pickle_save_dir_prefix}_{particle_type}{train_data_id_title}/all_alt_data_splits_{fit_type}_{num_bins_SR}_0_501\", \"rb\") as ifile:\n",
    "    loc_all_alt_data_splits_0 = pickle.load(ifile)\n",
    "with open(f\"{pickle_save_dir_prefix}_{particle_type}{train_data_id_title}/all_alt_data_splits_{fit_type}_{num_bins_SR}_501_1001\", \"rb\") as ifile:\n",
    "    loc_all_alt_data_splits_1 = pickle.load(ifile)\n",
    "all_alt_data_splits = {**loc_all_alt_data_splits_0, **loc_all_alt_data_splits_1}\n",
    "print(len(all_alt_data_splits.keys())==num_pseudoexperiments)\n",
    "# alt scores\n",
    "with open(f\"{pickle_save_dir_prefix}_{particle_type}{train_data_id_title}/all_alt_scores_splits_{fit_type}_{num_bins_SR}_0_501\", \"rb\") as ifile:\n",
    "    loc_all_alt_scores_splits_0 = pickle.load(ifile)\n",
    "with open(f\"{pickle_save_dir_prefix}_{particle_type}{train_data_id_title}/all_alt_scores_splits_{fit_type}_{num_bins_SR}_501_1001\", \"rb\") as ifile:\n",
    "    loc_all_alt_scores_splits_1 = pickle.load(ifile)\n",
    "all_alt_scores_splits = {**loc_all_alt_scores_splits_0, **loc_all_alt_scores_splits_1}\n",
    "print(len(all_alt_scores_splits.keys())==num_pseudoexperiments)\n",
    "\n",
    "\n",
    "with open(f\"{working_dir}/processed_data/mass_scaler_{particle_type}\", \"rb\") as ifile:\n",
    "    scaler = pickle.load(ifile)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef53bfa-2988-4ecc-909f-f722394c2f21",
   "metadata": {},
   "source": [
    "## Plot histograms for a small number of FPR thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b4b449-84f6-4b4b-bdec-66be0ce0a8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fpr_thresholds = [1, 0.25, 0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "\n",
    "# determine score cutoffs for each pseudoexperiments\n",
    "score_cutoffs = {pseudo_e:{i:{threshold:0 for threshold in fpr_thresholds} for i in range(n_folds)} for pseudo_e in range(1)}\n",
    "\n",
    "for pseudo_e in range(1):\n",
    "    for i_fold in range(n_folds):\n",
    "        \n",
    "        loc_scores_sorted = np.sort(1.0-all_alt_scores_splits[pseudo_e][\"FPR_validation\"][i_fold])\n",
    "        \n",
    "        for threshold in fpr_thresholds:\n",
    "            \n",
    "            loc_score_cutoff = 1-loc_scores_sorted[min(int(threshold*len(loc_scores_sorted)),len(loc_scores_sorted)-1)]\n",
    "            score_cutoffs[pseudo_e][i_fold][threshold] = loc_score_cutoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1079e4d4-69df-4b01-99e8-caccc935c86e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_histograms_with_fits(fpr_thresholds, data_dict_by_fold, scores_dict_by_fold, score_cutoffs_by_fold, mass_scalar, fit_type, num_bins_SR, title, SB_left, SR_left, SR_right, SB_right, n_folds= 5, take_score_avg=True):\n",
    "    \n",
    "    if fit_type == \"cubic\": fit_function = bkg_fit_cubic\n",
    "    elif fit_type == \"quintic\": fit_function = bkg_fit_quintic\n",
    "    elif fit_type == \"septic\": fit_function = bkg_fit_septic\n",
    "\n",
    "    # define bins and bin edges for the SB and SR\n",
    "    # change the bin width with `num_bins_SR`\n",
    "    plot_bins_all, plot_bins_SR, plot_bins_left, plot_bins_right, plot_centers_all, plot_centers_SR, plot_centers_SB = get_bins(SR_left, SR_right, SB_left, SB_right, num_bins_SR = num_bins_SR)\n",
    "    width = plot_bins_SR[1] - plot_bins_SR[0]\n",
    "    center = 0.5*(plot_bins_SR[0] + plot_bins_SR[-1])\n",
    "    mass_reso = 1.1*center\n",
    "    print(width, center, 100*width/center)\n",
    "    fig, ax = newplot(\"full\", width = 12, height = 9, use_tex = latex_flag)\n",
    "    for t, threshold in enumerate(fpr_thresholds):\n",
    "        \n",
    "        filtered_masses = []\n",
    "\n",
    "        # for each fold, select the events that meet the fpr threshold\n",
    "        for i_fold in range(n_folds):\n",
    "            loc_true_masses = mass_scalar.inverse_transform(np.array(data_dict_by_fold[i_fold][:,-1]).reshape(-1,1))\n",
    "            print(loc_true_masses.shape)\n",
    "            if take_score_avg:\n",
    "                loc_scores = np.mean(scores_dict_by_fold[i_fold], axis = 1)\n",
    "            else:\n",
    "                loc_scores = scores_dict_by_fold[i_fold]\n",
    "            print(max(loc_scores))\n",
    "            loc_filtered_masses, loc_SBL_eff, loc_SBH_eff, loc_SR_eff = select_top_events_fold(loc_true_masses, loc_scores, score_cutoffs_by_fold[i_fold][threshold],plot_bins_left, plot_bins_right, plot_bins_SR)\n",
    "            filtered_masses.append(loc_filtered_masses)\n",
    "        # consolidate the fold information\n",
    "        filtered_masses = np.concatenate(filtered_masses)\n",
    "        print(filtered_masses.shape)\n",
    "\n",
    "        # get the fit function to SB background\n",
    "        popt, pcov, chi2, y_vals, n_dof = curve_fit_m_inv(filtered_masses, fit_type, SR_left, SR_right, plot_bins_left, plot_bins_right, plot_centers_SB)\n",
    "        #print(\"chi2/dof:\", chi2/n_dof)\n",
    "        \n",
    "        # plot the fit function\n",
    "        plt.plot(plot_centers_all, fit_function(plot_centers_all, *popt), lw = 2, linestyle = \"dashed\", color = f\"C{t}\")    \n",
    "        function_with_band(ax, fit_function, [SB_left, SB_right], popt, pcov, color = f\"C{t}\")\n",
    "\n",
    "# def function_with_band(ax, f, range, params, pcov = None, color = \"purple\", alpha_line = 0.75, alpha_band = 0.25, lw = 3,  **kwargs):\n",
    "\n",
    "\n",
    "        # calculate significance of bump\n",
    "        num_S_expected_in_SR, num_B_expected_in_SR = calc_significance(filtered_masses, fit_function, plot_bins_SR, plot_centers_SR, SR_left, SR_right, popt)\n",
    "        y_err = get_errors_bkg_fit_ratio(popt, pcov, plot_centers_SR, fit_type)\n",
    "        B_error = np.sqrt(np.sum(y_err**2))\n",
    "        print(f\"B expected: {num_B_expected_in_SR}. B error: {B_error}\")\n",
    "        S_over_B = num_S_expected_in_SR/num_B_expected_in_SR\n",
    "        significance = num_S_expected_in_SR/np.sqrt(num_B_expected_in_SR+B_error**2)\n",
    "\n",
    "        label_string = str(round(100*threshold, 2))+\"% FPR: $S/B$: \"+str(round(S_over_B,2))+\", $S/\\sqrt{B}$: \"+str(round(significance,2))\n",
    "\n",
    "        # hist_with_errors(ax, filtered_masses, bins = plot_bins_all, range = (SB_left, SB_right), lw = 3, color = f\"C{t}\",label = label_string)\n",
    "        # hist_with_outline(ax, filtered_masses, bins = plot_bins_all, range = (SB_left, SB_right), lw = 3, color = f\"C{t}\",label = label_string)\n",
    "        plt.hist(filtered_masses, bins = plot_bins_all, lw = 3, histtype = \"step\", color = f\"C{t}\",label = label_string, alpha = 0.75)\n",
    "        plt.scatter(plot_centers_SB, y_vals, color = f\"C{t}\")\n",
    "\n",
    "\n",
    "\n",
    "    legend_title = r\"Upsilon Resonances: Iso, Opp. Sign\"\n",
    "    plt.legend(loc = (0.975, 0.6), fontsize = 16, title = title[:-1])\n",
    "\n",
    "\n",
    "    plt.axvline(SR_left, color= \"k\", lw = 3, zorder = 10)\n",
    "    plt.axvline(SR_right, color= \"k\", lw = 3, zorder = 10)\n",
    "\n",
    "    plt.xlabel(\"$M_{\\mu\\mu}$ [GeV]\", fontsize = 18)\n",
    "    plt.ylabel(\"Events\", fontsize = 18)\n",
    "\n",
    "    #plt.yscale(\"log\")\n",
    "    #plt.ylim(0.5, 1e5)\n",
    "\n",
    "    # Add more x ticks (major and minor)\n",
    "    plt.xticks(fontsize = 18)\n",
    "    plt.yticks(fontsize = 18)\n",
    "    plt.minorticks_on()\n",
    "    plt.tick_params(axis='x', which='minor', bottom=True)\n",
    "    plt.tick_params(axis='y', which='minor', left=True)\n",
    "    \n",
    "\n",
    "    # # Vertical Black Lines at boundaries of SR\n",
    "    # plt.axvline(SR_left, color = \"black\", linestyle = \"--\", lw = 2)\n",
    "    # plt.axvline(SR_right, color = \"black\", linestyle = \"--\", lw = 2)\n",
    "\n",
    "\n",
    "    # plt.title(title, fontsize = 24)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4b6d8-a877-4ba0-99af-c3074f3ddbe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(score_cutoffs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae69a3b-9910-4511-a6c1-61884b8b27de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "PLOT HISTOGRAM ON SMALL TEST SET\n",
    "\"\"\"\n",
    "plot_histograms_with_fits(fpr_thresholds, all_test_data_splits[pseudo_e_to_plot], all_scores_splits[pseudo_e_to_plot], \n",
    "                          score_cutoffs[pseudo_e_to_plot], scaler, fit_type, num_bins_SR,\n",
    "                          f\"{particle_type}{train_data_id_title} (trained on {train_data_id_title})\\n\", \n",
    "                          SB_left, SR_left, SR_right, SB_right, take_score_avg=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d54a2f-96e3-45b6-8d4c-acb5385032ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "PLOT HISTOGRAM ON ALTERNATIVE TEST SET\n",
    "\"\"\"\n",
    "plot_histograms_with_fits(fpr_thresholds, all_alt_data_splits[pseudo_e_to_plot][\"alt\"], \n",
    "                          all_alt_scores_splits[pseudo_e_to_plot][\"alt\"], score_cutoffs[pseudo_e_to_plot], scaler, \n",
    "                          fit_type, num_bins_SR,\n",
    "                          f\"{particle_type}{alt_test_data_id} (trained on {train_data_id_title})\\n\",SB_left, \n",
    "                          SR_left, SR_right, SB_right, take_score_avg=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349e4ec0-06dd-4976-b8d1-b85c961119b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PLOT HISTOGRAM ON FLOW SAMPLES\n",
    "\"\"\"\n",
    "plot_histograms_with_fits(fpr_thresholds, all_alt_data_splits[pseudo_e_to_plot][\"ROC_samples\"], \n",
    "                          all_alt_scores_splits[pseudo_e_to_plot][\"ROC_samples\"], score_cutoffs[pseudo_e_to_plot], scaler, \n",
    "                          fit_type,num_bins_SR,\n",
    "                          f\"high-stats samples {train_data_id_title} (trained on {train_data_id_title})\\n\", \n",
    "                          SB_left, SR_left, SR_right, SB_right, take_score_avg=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3da91d-1b52-437e-9e1b-a5d612e81c31",
   "metadata": {},
   "source": [
    "# Plot ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f03938d-7a21-44ef-a26d-fe6f147414e4",
   "metadata": {},
   "source": [
    "Calculate the ROC curves for true S / B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a99712b-a3a2-4567-991c-905debf19f20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# determine fpr thresholds as before\n",
    "# yes this is repeated code\n",
    "fpr_thresholds_finegrained = np.logspace(0, -3, 50)\n",
    "#fpr_thresholds = np.linspace(1, 0 , 50)\n",
    "\n",
    "plot_bins_all, plot_bins_SR, plot_bins_left, plot_bins_right, plot_centers_all, plot_centers_SR, plot_centers_SB = get_bins(SR_left, SR_right, SB_left, SB_right, num_bins_SR = num_bins_SR)\n",
    "\n",
    "\n",
    "# first determine score cutoffs\n",
    "score_cutoffs_finegrained = {pseudo_e:{i:{threshold:0 for threshold in fpr_thresholds_finegrained} for i in range(n_folds)} for pseudo_e in range(num_pseudoexperiments)}\n",
    "\n",
    "for pseudo_e in range(num_pseudoexperiments):\n",
    "    for i_fold in range(n_folds):\n",
    "        loc_scores_sorted = np.sort(1.0-all_alt_scores_splits[pseudo_e][\"FPR_validation\"][i_fold])\n",
    "        for threshold in fpr_thresholds_finegrained:\n",
    "            loc_score_cutoff = 1-loc_scores_sorted[min(int(threshold*len(loc_scores_sorted)),len(loc_scores_sorted)-1)]\n",
    "            score_cutoffs_finegrained[pseudo_e][i_fold][threshold] = loc_score_cutoff\n",
    "\n",
    "        \n",
    "def get_classifier_metrics_high_stats(dataset_by_pseudo_e, scores_by_pseudo_e, score_cutoffs):\n",
    "            \n",
    "    S_yield, B_yield = np.empty((fpr_thresholds_finegrained.shape[0], num_pseudoexperiments)), np.empty((fpr_thresholds_finegrained.shape[0], num_pseudoexperiments))\n",
    "\n",
    "    for pseudo_e in range(num_pseudoexperiments):\n",
    "\n",
    "        print(f\"On pseudo experiment {pseudo_e}...\")\n",
    "        for t, threshold in enumerate(fpr_thresholds_finegrained):\n",
    "\n",
    "            filtered_masses_bs = []\n",
    "\n",
    "            for i_fold in range(n_folds):\n",
    "                loc_true_masses_bs = scaler.inverse_transform(np.array(dataset_by_pseudo_e[pseudo_e][i_fold][:,-1]).reshape(-1,1))\n",
    "                loc_scores_bs = scores_by_pseudo_e[pseudo_e][i_fold]\n",
    "                # filter top event based on score cutoff\n",
    "                loc_filtered_masses_bs, _, _, _ = select_top_events_fold(loc_true_masses_bs, loc_scores_bs, score_cutoffs[pseudo_e][i_fold][threshold], plot_bins_left, plot_bins_right, plot_bins_SR)\n",
    "                filtered_masses_bs.append(loc_filtered_masses_bs)\n",
    "\n",
    "            filtered_masses_bs = np.concatenate(filtered_masses_bs)\n",
    "            # get the fit function to SB background\n",
    "            popt, pcov, chi2, y_vals, n_dof = curve_fit_m_inv(filtered_masses_bs, fit_type, SR_left, SR_right, plot_bins_left, plot_bins_right, plot_centers_SB)\n",
    "            num_S_expected_in_SR, num_B_expected_in_SR = calc_significance(filtered_masses_bs, fit_function, plot_bins_SR, plot_centers_SR, SR_left, SR_right, popt)\n",
    "\n",
    "            y_err = get_errors_bkg_fit_ratio(popt, pcov, plot_centers_SR, fit_type)\n",
    "            B_error = np.sqrt(np.sum(y_err**2))\n",
    "            S_over_B = num_S_expected_in_SR/num_B_expected_in_SR\n",
    "            \n",
    "            \n",
    "            significance = num_S_expected_in_SR/np.sqrt(num_B_expected_in_SR+B_error**2)\n",
    "\n",
    "            # TODO: ERRORS\n",
    "            \n",
    "            S_yield[t, pseudo_e] = num_S_expected_in_SR\n",
    "            B_yield[t, pseudo_e] = num_B_expected_in_SR\n",
    "        \n",
    "    # calculate summary stats\n",
    "    TPR = S_yield/S_yield[0,:]\n",
    "    FPR = B_yield/B_yield[0,:]\n",
    "    \n",
    "    \n",
    "\n",
    "    ROC = 1.0/FPR\n",
    "\n",
    "    SIC = TPR/np.sqrt(FPR)\n",
    "    \n",
    "    return TPR, FPR, ROC, SIC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec045d1c-a6ca-4d89-95aa-c9def41d039a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TPR, FPR, ROC, SIC = get_classifier_metrics_high_stats(all_test_data_splits, all_scores_splits, score_cutoffs_finegrained)\n",
    "\n",
    "TPR_median, TPR_lower, TPR_upper = get_median_percentiles(TPR)\n",
    "FPR_median, FPR_lower, FPR_upper = get_median_percentiles(FPR)\n",
    "ROC_median, ROC_lower, ROC_upper = get_median_percentiles(ROC)\n",
    "SIC_median, SIC_lower, SIC_upper = get_median_percentiles(SIC)\n",
    "\n",
    "all_TPR[(fit_type, num_bins_SR)] = TPR_median, TPR_lower, TPR_upper\n",
    "all_FPR[(fit_type, num_bins_SR)] = FPR_median, FPR_lower, FPR_upper\n",
    "all_ROC[(fit_type, num_bins_SR)] = ROC_median, ROC_lower, ROC_upper\n",
    "all_SIC[(fit_type, num_bins_SR)] = SIC_median, SIC_lower, SIC_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63114e3f-f906-48c8-8926-52865fc030cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "colors_dict = {\"cubic\": \"red\", \"quintic\":\"blue\", \"septic\":\"purple\"}\n",
    "styles_dict = {16:\"solid\", 12:\"dashed\", 8:\"dotted\"}\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for ff, nn in all_TPR.keys():\n",
    "    plt.plot(all_FPR[(ff,nn)][0], all_TPR[(ff,nn)][0], label=f\"{ff}, {nn}\", color = colors_dict[ff], linestyle=styles_dict[nn])\n",
    "    plt.fill_between(all_FPR[(ff,nn)][0], all_TPR[(ff,nn)][1], all_TPR[(ff,nn)][2], alpha = 0.2, color = colors_dict[ff] )\n",
    "    \n",
    "plt.plot(all_FPR[(ff,nn)][0], all_FPR[(ff,nn)][0], linestyle = \"dashed\", color = \"grey\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.legend(loc=(1,0))\n",
    "plt.ylim(-1,6)\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(f\"low-stats data _oppsign (trained on {train_data_id_title})\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cfeaa3-6968-402e-9589-101d2a03abfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = newplot(\"full\", use_tex = latex_flag)\n",
    "for ff, nn in all_TPR.keys():\n",
    "    \n",
    "    ax.plot(all_FPR[(ff,nn)][0], all_SIC[(ff,nn)][0], label=f\"{ff}, {nn}\", color = colors_dict[ff], linestyle=styles_dict[nn])\n",
    "    ax.fill_between(all_FPR[(ff,nn)][0], all_SIC[(ff,nn)][1], all_SIC[(ff,nn)][2], alpha = 0.2, color = colors_dict[ff] )\n",
    "ax.plot(all_FPR[(ff,nn)][0], all_FPR[(ff,nn)][0]/np.sqrt(all_FPR[(ff,nn)][0]), linestyle = \"dashed\", color = \"grey\")\n",
    "plt.legend(loc=(1,0))\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"FPR\")\n",
    "ax.set_ylim(-1,30)\n",
    "ax.set_ylabel(\"$S/\\sqrt{B}$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c078a-1654-4aee-9cde-f0d20fa8aaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53d9e124-fe68-4fe2-8051-d17e8d4ec98c",
   "metadata": {},
   "source": [
    "Calculate the ROC curves for data vs cathode samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a70765-7c75-4a97-bfa0-89e2aa0b6d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_interp = np.linspace(1e-5, 1, 100)\n",
    "TPR = np.zeros((fpr_interp.shape[0], num_pseudoexperiments))\n",
    "\n",
    "for pseudo_e in range(num_pseudoexperiments):\n",
    "\n",
    "    print(f\"On pseudo experiment {pseudo_e}...\")\n",
    "    \n",
    "    scores_pseudo_e_data = []\n",
    "    scores_pseudo_e_samples = []\n",
    "\n",
    "    for i_fold in range(n_folds):\n",
    "        scores_pseudo_e_data.append(all_scores_splits[pseudo_e][i_fold])\n",
    "        scores_pseudo_e_samples.append(all_alt_scores_splits[pseudo_e][\"FPR_validation\"][i_fold])\n",
    "\n",
    "    scores_pseudo_e_data = np.concatenate(scores_pseudo_e_data)\n",
    "    scores_pseudo_e_samples = np.concatenate(scores_pseudo_e_samples)\n",
    "\n",
    "    scores_all = np.hstack([scores_pseudo_e_data,scores_pseudo_e_samples])\n",
    "    labels_all = np.hstack([np.ones((scores_pseudo_e_data.shape[0],)),np.zeros((scores_pseudo_e_samples.shape[0],))])\n",
    "    \n",
    "    loc_fpr, loc_tpr, _ = roc_curve(labels_all, scores_all)\n",
    "    tpr_interp = np.interp(fpr_interp, loc_fpr, loc_tpr)\n",
    "    \n",
    "    TPR[:,pseudo_e] = tpr_interp\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63527dd-435e-4deb-890e-450ed0ee2625",
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR_median, TPR_lower, TPR_upper = get_median_percentiles(TPR)\n",
    "\n",
    "plt.plot(fpr_interp,TPR_median, lw = 0.5)\n",
    "plt.fill_between(fpr_interp, TPR_lower, TPR_upper, alpha = 0.2)\n",
    "\n",
    "plt.plot(fpr_interp,fpr_interp, linestyle = \"dashed\", color = \"grey\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b253df6-2758-49dc-8719-32e6b1c927c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bedd38-9e0b-4d53-b50e-3baa07b6c075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef413bc7-48f6-4808-a212-8e04e714af5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc5ab5-8c4d-475f-82ef-5762b98f8a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d47a3-053f-48e6-b4e2-6c005e71d104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
