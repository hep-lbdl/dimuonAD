{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c70a61a3-13b2-44af-b9aa-1292b922a7fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "from helpers.physics_functions import bkg_fit_cubic, bkg_fit_septic, bkg_fit_quintic, get_bins, select_top_events_fold, curve_fit_m_inv, calculate_test_statistic, get_errors_bkg_fit_ratio\n",
    "from helpers.evaluation import get_median_percentiles\n",
    "from helpers.plotting import newplot, hist_with_outline, hist_with_errors, function_with_band\n",
    "from helpers.data_transforms import scaled_to_physical_transform\n",
    "# Try to load LaTeX\n",
    "latex_flag = False\n",
    "np.seterr(divide='ignore')\n",
    "plt.style.use(\"../science.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9131a5-14ad-48b6-af76-17d7b0ac5b12",
   "metadata": {},
   "source": [
    "This notebook should be run twice:\n",
    "\n",
    "1. `train_samesign = False` gives the \"standard\" results. i.e. we run the studies on the OS samples\n",
    "2. `train_samesign = True` comes from running the Ml study on the SS samples.\n",
    "\n",
    "**CAUTION**: for the histograms, we are truly showing the significance as $\\frac{S}{\\sqrt{B+{\\sigma_B}^2}}$, i.e. we are accounting for the background error. For the ROC curves, this error is *NOT* being taken into account (it's not clear to me that we want this background error when we are just citing the background yield for the FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad6c7ec-8adf-4883-b5c7-0f0a02043d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"workflow.yaml\", \"r\") as file:\n",
    "    workflow = yaml.safe_load(file) \n",
    "    \n",
    "feature_id = \"mix_2\"\n",
    "bootstrap_flow = 0 # don't change this from 0\n",
    "\n",
    "train_samesign = False\n",
    "# somewhat complicated code to set naming conventions\n",
    "if train_samesign:\n",
    "    train_data_id = \"SS\"\n",
    "else:\n",
    "    train_data_id = \"OS\"\n",
    "\n",
    "# train on opp sign means alt test set is samesign\n",
    "if train_data_id == \"OS\": \n",
    "    alt_test_data_id = \"SS\"\n",
    "elif train_data_id == \"SS\": \n",
    "    alt_test_data_id = \"OS\"\n",
    "\n",
    "configs = \"CATHODE_8\"\n",
    "\n",
    "\n",
    "# pickles contain all the results from the BDT training\n",
    "working_dir = workflow[\"file_paths\"][\"working_dir\"]\n",
    "processed_data_dir = workflow[\"file_paths\"][\"data_storage_dir\"] +\"/projects/\"+workflow[\"analysis_keywords\"][\"name\"]+\"/processed_data\"\n",
    "flow_training_dir = workflow[\"file_paths\"][\"data_storage_dir\"] +\"/projects/\" + workflow[\"analysis_keywords\"][\"name\"]+f\"/models/bootstrap{bootstrap_flow}_{train_data_id}/{feature_id}/{configs}/\"\n",
    "pickle_save_dir = workflow[\"file_paths\"][\"data_storage_dir\"] +\"/projects/\" + workflow[\"analysis_keywords\"][\"name\"]+f\"/pickles/bootstrap{bootstrap_flow}_{train_data_id}/{feature_id}/\"\n",
    "\n",
    "# basically hard-coded for the PRL \n",
    "num_pseudoexperiments = 1001\n",
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ace8c3d0-83b9-4747-a9c3-fe154828cad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_bins_SR = 12 # 16, 12, 8\n",
    "\n",
    "pseudo_e_to_plot = 0 # this plots the actual data (not a boostrapped version)\n",
    "fit_type = \"quintic\" # \"cubic\", \"quintic\", septic\n",
    "if fit_type == \"cubic\": fit_function = bkg_fit_cubic\n",
    "if fit_type == \"quintic\": fit_function = bkg_fit_quintic\n",
    "if fit_type == \"septic\": fit_function = bkg_fit_septic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a73e9288-aed1-4383-959a-cbbc5ae3c3d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0 16.0\n"
     ]
    }
   ],
   "source": [
    "SB_left = float(workflow[\"window_definitions\"][workflow[\"analysis_keywords\"][\"particle\"]][\"SB_left\"])\n",
    "SR_left = float(workflow[\"window_definitions\"][workflow[\"analysis_keywords\"][\"particle\"]][\"SR_left\"])\n",
    "SR_right = float(workflow[\"window_definitions\"][workflow[\"analysis_keywords\"][\"particle\"]][\"SR_right\"])\n",
    "SB_right = float(workflow[\"window_definitions\"][workflow[\"analysis_keywords\"][\"particle\"]][\"SB_right\"])\n",
    "\n",
    "print(SB_left, SB_right)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d54f5",
   "metadata": {},
   "source": [
    "# Load in the data / BDT results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c92df6ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Set: ['dimu_pt', 'mu0_ip3d', 'mu1_ip3d', 'dimu_mass']\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{flow_training_dir}/seed1/configs.txt\", \"rb\") as infile: \n",
    "    configs = infile.readlines()[0].decode(\"utf-8\")\n",
    "    feature_set = [x.strip() for x in configs.split(\"'\")][1::2]\n",
    "\n",
    "print(f\"Feature Set: {feature_set}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66841771-559c-47ad-aa75-7c7719d4747b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# if train_samesign = False, this loads in the OS test data\n",
    "# test \n",
    "\n",
    "\n",
    "def load_in_pseudoexperiments(file_string, num_pseudoexps):\n",
    "\n",
    "    master_dict = {}\n",
    "\n",
    "    with open(f\"{pickle_save_dir}/{file_string}_{fit_type}_{num_bins_SR}_0_1\", \"rb\") as ifile:\n",
    "        loc_dict = pickle.load(ifile)\n",
    "    master_dict = {**loc_dict}\n",
    "    # load in the bootstraps\n",
    "    for i in range(1, num_pseudoexps):\n",
    "        with open(f\"{pickle_save_dir}/bkg_samples/bootstrap{i}/{file_string}_{fit_type}_{num_bins_SR}_0_1\", \"rb\") as ifile:\n",
    "            loc_dict = pickle.load(ifile)\n",
    "            master_dict[i] = loc_dict[0]\n",
    "    return master_dict\n",
    "\n",
    "num_to_plot = 4\n",
    "\n",
    "all_test_data_splits = load_in_pseudoexperiments(\"all_test_data_splits\", num_to_plot)\n",
    "print(len(all_test_data_splits.keys())==num_pseudoexperiments)\n",
    "\n",
    "# test scores\n",
    "all_scores_splits = load_in_pseudoexperiments(\"all_scores_splits\", num_to_plot)\n",
    "print(len(all_scores_splits.keys())==num_pseudoexperiments)\n",
    "\n",
    "# alt data\n",
    "# if train_samesign = False, this loads in the SS test data, OS high-stats data, and OS flow samples\n",
    "# if train_samesign = True, this loads in the OS test data, SS high-stats data, and SS flow samples\n",
    "all_alt_data_splits = load_in_pseudoexperiments(\"all_alt_data_splits\", num_to_plot)\n",
    "print(len(all_alt_data_splits.keys())==num_pseudoexperiments)\n",
    "# alt scores\n",
    "all_alt_scores_splits = load_in_pseudoexperiments(\"all_alt_scores_splits\", num_to_plot)\n",
    "\n",
    "print(len(all_alt_scores_splits.keys())==num_pseudoexperiments)\n",
    "\n",
    "with open(f\"{processed_data_dir}/mass_scaler_bootstrap{bootstrap_flow}\", \"rb\") as ifile:\n",
    "    old_scaler = pickle.load(ifile)\n",
    "    \n",
    "with open(f\"{processed_data_dir}/preprocessing_info_bootstrap{bootstrap_flow}\", \"rb\") as ifile:\n",
    "     preprocessing_info = pickle.load(ifile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef53bfa-2988-4ecc-909f-f722394c2f21",
   "metadata": {},
   "source": [
    "## Plot histograms for a small number of FPR thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b4b449-84f6-4b4b-bdec-66be0ce0a8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fpr_thresholds = [1, 0.75, 0.5, 0.25, 0.1, 0.05, 0.01]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# determine score cutoffs for each pseudoexperiments\n",
    "score_cutoffs = {pseudo_e:{i:{threshold:0 for threshold in fpr_thresholds} for i in range(n_folds)} for pseudo_e in range(num_to_plot)}\n",
    "\n",
    "for pseudo_e in range(num_to_plot):\n",
    "    for i_fold in range(n_folds):\n",
    "        \n",
    "        loc_scores_sorted = np.sort(1.0-all_alt_scores_splits[pseudo_e][\"FPR_validation\"][i_fold])\n",
    "        \n",
    "        for threshold in fpr_thresholds:\n",
    "            \n",
    "            loc_score_cutoff = 1-loc_scores_sorted[min(int(threshold*len(loc_scores_sorted)),len(loc_scores_sorted)-1)]\n",
    "            score_cutoffs[pseudo_e][i_fold][threshold] = loc_score_cutoff\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1079e4d4-69df-4b01-99e8-caccc935c86e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_histograms_with_fits_SR(fpr_thresholds, data_dict_by_fold, scores_dict_by_fold, score_cutoffs_by_fold, mass_scalar, fit_type, num_bins_SR, title, SB_left, SR_left, SR_right, SB_right, n_folds= 5, take_score_avg=True):\n",
    "    \n",
    "    if fit_type == \"cubic\": fit_function = bkg_fit_cubic\n",
    "    elif fit_type == \"quintic\": fit_function = bkg_fit_quintic\n",
    "    elif fit_type == \"septic\": fit_function = bkg_fit_septic\n",
    "\n",
    "    # define bins and bin edges for the SB and SR\n",
    "    # change the bin width with `num_bins_SR`\n",
    "    plot_bins_all, plot_bins_SR, plot_bins_left, plot_bins_right, plot_centers_all, plot_centers_SR, plot_centers_SB = get_bins(SR_left, SR_right, SB_left, SB_right, num_bins_SR = num_bins_SR)\n",
    " \n",
    "    fig, ax = newplot(\"full\", width = 12, height = 9, use_tex = latex_flag)\n",
    "    for t, threshold in enumerate(fpr_thresholds):\n",
    "        \n",
    "        filtered_masses = []\n",
    "\n",
    "        # for each fold, select the events that meet the fpr threshold\n",
    "        for i_fold in range(n_folds):\n",
    "            #loc_true_masses = mass_scalar(np.array(data_dict_by_fold[i_fold][:,-1]).reshape(-1,1))\n",
    "            loc_true_masses = mass_scalar.inverse_transform(np.array(data_dict_by_fold[i_fold][:,-1]).reshape(-1,1))\n",
    "            \n",
    "            if take_score_avg:\n",
    "                loc_scores = np.mean(scores_dict_by_fold[i_fold], axis = 1)\n",
    "            else:\n",
    "                loc_scores = scores_dict_by_fold[i_fold]\n",
    "            \n",
    "            \n",
    "            loc_filtered_masses, loc_SBL_eff, loc_SBH_eff, loc_SR_eff = select_top_events_fold(loc_true_masses, loc_scores, score_cutoffs_by_fold[i_fold][threshold],plot_bins_left, plot_bins_right, plot_bins_SR)\n",
    "            \n",
    "            filtered_masses.append(loc_filtered_masses)\n",
    "        # consolidate the fold information\n",
    "        filtered_masses = np.concatenate(filtered_masses)\n",
    "    \n",
    "        # get the fit function to SB background\n",
    "        popt, pcov, chi2, y_vals, n_dof = curve_fit_m_inv(filtered_masses, fit_type, SR_left, SR_right, plot_bins_left, plot_bins_right, plot_centers_SB)\n",
    "        #print(\"chi2/dof:\", chi2/n_dof)\n",
    "        \n",
    "        # plot the fit function\n",
    "        plt.plot(plot_centers_all, fit_function(plot_centers_all, *popt), lw = 2, linestyle = \"dashed\", color = f\"C{t}\")    \n",
    "        function_with_band(ax, fit_function, [SB_left, SB_right], popt, pcov, color = f\"C{t}\")\n",
    "\n",
    "# def function_with_band(ax, f, range, params, pcov = None, color = \"purple\", alpha_line = 0.75, alpha_band = 0.25, lw = 3,  **kwargs):\n",
    "\n",
    "\n",
    "        # calculate significance of bump\n",
    "        S, B, q0 = calculate_test_statistic(filtered_masses, fit_function, fit_type, plot_bins_SR, plot_centers_SR, SR_left, SR_right, popt, pcov)\n",
    "       \n",
    "        significance = np.sqrt(q0)\n",
    "\n",
    "        label_string = str(round(100*threshold, 2))+\" S: \" +str(round(S,0)) + \", B: \" +str(round(B, 0)) + \" Z: \"+str(round(significance,2))\n",
    "\n",
    "        # hist_with_errors(ax, filtered_masses, bins = plot_bins_all, range = (SB_left, SB_right), lw = 3, color = f\"C{t}\",label = label_string)\n",
    "        # hist_with_outline(ax, filtered_masses, bins = plot_bins_all, range = (SB_left, SB_right), lw = 3, color = f\"C{t}\",label = label_string)\n",
    "        plt.hist(filtered_masses, bins = plot_bins_all, lw = 3, histtype = \"step\", color = f\"C{t}\",label = label_string, alpha = 0.75)\n",
    "        plt.scatter(plot_centers_SB, y_vals, color = f\"C{t}\")\n",
    "\n",
    "\n",
    "\n",
    "    legend_title = f\"Upsilon Resonances: Iso, Opp. Sign {feature_id}\"\n",
    "    plt.legend(loc = (0.975, 0.6), fontsize = 16, title = title[:-1])\n",
    "\n",
    "\n",
    "    plt.axvline(SR_left, color= \"k\", lw = 3, zorder = 10)\n",
    "    plt.axvline(SR_right, color= \"k\", lw = 3, zorder = 10)\n",
    "\n",
    "    plt.xlabel(\"$M_{\\mu\\mu}$ [GeV]\", fontsize = 18)\n",
    "    plt.ylabel(\"Events\", fontsize = 18)\n",
    "\n",
    "    #plt.yscale(\"log\")\n",
    "    plt.ylim(0, 300)\n",
    "\n",
    "    # Add more x ticks (major and minor)\n",
    "    plt.xticks(fontsize = 18)\n",
    "    plt.yticks(fontsize = 18)\n",
    "    plt.minorticks_on()\n",
    "    plt.tick_params(axis='x', which='minor', bottom=True)\n",
    "    plt.tick_params(axis='y', which='minor', left=True)\n",
    "    \n",
    "\n",
    "    # # Vertical Black Lines at boundaries of SR\n",
    "    # plt.axvline(SR_left, color = \"black\", linestyle = \"--\", lw = 2)\n",
    "    # plt.axvline(SR_right, color = \"black\", linestyle = \"--\", lw = 2)\n",
    "\n",
    "\n",
    "    plt.title(title, fontsize = 24)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93024977-0927-4b58-a269-6d79e6bb3994",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def plot_features_corner(fpr_thresholds, data_dict_by_fold, scores_dict_by_fold, score_cutoffs_by_fold, mass_scalar, SR_left, SR_right, n_folds= 5, take_score_avg=True):\n",
    "\n",
    "    n_features = len(feature_set) - 1\n",
    "    nbins = 50\n",
    "\n",
    "    bins = {\n",
    "        \"dimu_pt\": np.linspace(25, 100, nbins),\n",
    "        \"mu0_ip3d\": np.logspace(-4, np.log10(0.3), nbins),\n",
    "        \"mu1_ip3d\": np.logspace(-4, np.log10(0.3), nbins),\n",
    "    }\n",
    "    labels = {\n",
    "        \"dimu_pt\": \"$\\mu^-\\mu^+$ $p_T$ [GeV]\",\n",
    "        \"mu0_ip3d\": \"Hardest $\\mu$ 3D IP\",\n",
    "        \"mu1_ip3d\": \"Softer $\\mu$ 3D IP\",\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(n_features, n_features, figsize = (4*n_features, 4*n_features))\n",
    "    \n",
    "    for t, threshold in enumerate(fpr_thresholds[::-1]):\n",
    "\n",
    "        label_string = str(round(100*threshold, 2))+\"% FPR\"\n",
    "        \n",
    "        filtered_features = {i:[] for i in range(n_features)}\n",
    "\n",
    "        # for each fold, select the events that meet the fpr threshold\n",
    "        for i_fold in range(n_folds):\n",
    "            if take_score_avg:\n",
    "                loc_scores = np.mean(scores_dict_by_fold[i_fold], axis = 1)\n",
    "            else:\n",
    "                loc_scores = scores_dict_by_fold[i_fold]\n",
    "                \n",
    "            loc_true_masses = mass_scalar.inverse_transform(np.array(data_dict_by_fold[i_fold][:,-1]).reshape(-1,1))\n",
    "            \n",
    "            \n",
    "            loc_in_SR_indices =  (loc_true_masses >= SR_left) & (loc_true_masses <= SR_right) \n",
    "            loc_score_indices = (loc_scores >= score_cutoffs_by_fold[i_fold][threshold])\n",
    "            \n",
    "            loc_pass_indices = loc_in_SR_indices.reshape(-1,) & loc_score_indices.reshape(-1,)\n",
    "            \n",
    "            for i_feat in range(n_features):\n",
    "                # undo the scaling\n",
    "                selected_features = scaled_to_physical_transform(data_dict_by_fold[i_fold][:,i_feat], preprocessing_info[feature_set[i_feat]])\n",
    "                filtered_features[i_feat].append(selected_features[loc_pass_indices])\n",
    "                \n",
    "        for i_feat in range(n_features):\n",
    "            filtered_features[i_feat] = np.concatenate(filtered_features[i_feat])\n",
    "            \n",
    "       # PLOTTING   \n",
    "            \n",
    "        for i_feat in range(n_features):\n",
    "            for j_feat in range(i_feat, n_features):\n",
    "                \n",
    "\n",
    "                plot_x = j_feat\n",
    "                plot_y = i_feat\n",
    "\n",
    "                if plot_x == n_features-1:\n",
    "                    ax[plot_x, plot_y].set_xlabel(labels[feature_set[i_feat]])\n",
    "                else:\n",
    "                    ax[plot_x, plot_y].set_xticks([])\n",
    "                if plot_y == 0:\n",
    "                    ax[plot_x, plot_y].set_ylabel(labels[feature_set[j_feat]])\n",
    "                else:\n",
    "                    ax[plot_x, plot_y].set_yticks([])\n",
    "\n",
    "                if i_feat == j_feat:\n",
    "                    ax[plot_x, plot_y].hist( filtered_features[i_feat], bins=bins[feature_set[i_feat]], histtype = \"step\", label=label_string)\n",
    "                    ax[plot_x, plot_y].set_yscale(\"log\")\n",
    "                    #ax[plot_x, plot_y].set_xlabel(labels[feature_set[i_feat]])\n",
    "                    ax[plot_x, plot_y].set_ylim(1e-1, 2*1e2)\n",
    "\n",
    "                elif i_feat < j_feat:\n",
    "\n",
    "                    ax[plot_x, plot_y].hist2d(filtered_features[i_feat], filtered_features[j_feat], bins = [bins[feature_set[i_feat]], bins[feature_set[j_feat]]], norm=\"log\")\n",
    "                    #ax[plot_x, plot_y].set_xlabel(labels[feature_set[i_feat]])\n",
    "                    \n",
    "                else:\n",
    "                    ax[plot_x, plot_y].axis(\"off\")\n",
    "                   \n",
    "\n",
    "        \n",
    "\n",
    "    legend_title = r\"Upsilon Resonances: Iso, Opp. Sign\"\n",
    "    plt.legend(loc = (0.975, 0.4), fontsize = 16, title = legend_title[:-1])\n",
    "\n",
    "    #ax[0].set_ylabel(\"Events\", fontsize = 18)\n",
    "\n",
    "    # Add more x ticks (major and minor)\n",
    "    #plt.xticks(fontsize = 18)\n",
    "    #plt.yticks(fontsize = 18)\n",
    "    #plt.minorticks_on()\n",
    "    #plt.tick_params(axis='x', which='minor', bottom=True)\n",
    "    #plt.tick_params(axis='y', which='minor', left=True)\n",
    "    plt.subplots_adjust(wspace=0, hspace= 0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a023ece7-c58e-45cf-973f-c547fe5c94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(fpr_thresholds, data_dict_by_fold, scores_dict_by_fold, score_cutoffs_by_fold, mass_scalar, SR_left, SR_right, n_folds= 5, take_score_avg=True):\n",
    "\n",
    "    n_features = len(feature_set) - 1\n",
    "    nbins = 40\n",
    "\n",
    "    bins = {\n",
    "        \"dimu_pt\": np.linspace(0, 150, nbins),\n",
    "        \"mu0_ip3d\": np.logspace(-4, 0, nbins),\n",
    "        \"mu1_ip3d\": np.logspace(-4, 0, nbins),\n",
    "    }\n",
    "    labels = {\n",
    "        \"dimu_pt\": \"Dimuon $p_T$ [GeV]\",\n",
    "        \"mu0_ip3d\": \"Muon 1 IP3D [cm]\",\n",
    "        \"mu1_ip3d\": \"Muon 2 IP3D [cm]\",\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(1, n_features, figsize = (5*n_features, 5))\n",
    "    \n",
    "    for t, threshold in enumerate(fpr_thresholds):\n",
    "        \n",
    "        filtered_features = {i:[] for i in range(n_features)}\n",
    "\n",
    "        # for each fold, select the events that meet the fpr threshold\n",
    "        for i_fold in range(n_folds):\n",
    "            if take_score_avg:\n",
    "                loc_scores = np.mean(scores_dict_by_fold[i_fold], axis = 1)\n",
    "            else:\n",
    "                loc_scores = scores_dict_by_fold[i_fold]\n",
    "                \n",
    "            loc_true_masses = mass_scalar.inverse_transform(np.array(data_dict_by_fold[i_fold][:,-1]).reshape(-1,1))\n",
    "                 \n",
    "            loc_in_SR_indices =  (loc_true_masses >= SR_left) & (loc_true_masses <= SR_right) \n",
    "            loc_score_indices = (loc_scores >= score_cutoffs_by_fold[i_fold][threshold])\n",
    "            \n",
    "            loc_pass_indices = loc_in_SR_indices.reshape(-1,) & loc_score_indices.reshape(-1,)\n",
    "            \n",
    "            for i_feat in range(n_features):\n",
    "                # undo the scaling\n",
    "                selected_features = scaled_to_physical_transform(data_dict_by_fold[i_fold][:,i_feat], preprocessing_info[feature_set[i_feat]])\n",
    "                filtered_features[i_feat].append(selected_features[loc_pass_indices])\n",
    "                \n",
    "        for i_feat in range(n_features):\n",
    "            filtered_features[i_feat] = np.concatenate(filtered_features[i_feat])   \n",
    "\n",
    "        for i_feat in range(n_features):\n",
    "\n",
    "            label_string = str(round(100*threshold, 2))+\"% FPR\"\n",
    "\n",
    "\n",
    "            ax[i_feat].hist( filtered_features[i_feat], bins=bins[feature_set[i_feat]], lw = 3, histtype = \"step\", color = f\"C{t}\",label = label_string, alpha = 0.75)\n",
    "            ax[i_feat].set_yscale(\"log\")\n",
    "            if i_feat in [1, 2]:\n",
    "                ax[i_feat].set_xscale(\"log\")\n",
    "                ax[i_feat].set_xticks([1e-3, 1e-2, 1e-1])\n",
    "            ax[i_feat].set_xlabel(labels[feature_set[i_feat]])\n",
    "            ax[i_feat].set_ylim(1e-1, 5*1e2)\n",
    "\n",
    "            ax[i_feat].tick_params(labelsize = 16)\n",
    "\n",
    "            if i_feat > 0:\n",
    "                ax[i_feat].set_yticklabels([])\n",
    "\n",
    "\n",
    "           \n",
    "\n",
    "    legend_title = r\"Upsilon Resonances: Iso, Opp. Sign\"\n",
    "    plt.legend(loc = (1.05, 0.4), fontsize = 16)\n",
    "\n",
    "    ax[0].set_ylabel(\"Events\", fontsize = 18)\n",
    "    plt.subplots_adjust(wspace=0)\n",
    "\n",
    "    return fig\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ab63e-00fa-4da8-b82f-2aa168a4dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms_with_fits_SB(fpr_thresholds, data_dict_by_fold, scores_dict_by_fold, mass_scalar, fit_type, num_bins_SR, title, SB_left, SR_left, SR_right, SB_right, n_folds= 5, take_score_avg=True):\n",
    "    \n",
    "    if fit_type == \"cubic\": fit_function = bkg_fit_cubic\n",
    "    elif fit_type == \"quintic\": fit_function = bkg_fit_quintic\n",
    "    elif fit_type == \"septic\": fit_function = bkg_fit_septic\n",
    "\n",
    "    # define bins and bin edges for the SB and SR\n",
    "    # change the bin width with `num_bins_SR`\n",
    "    plot_bins_all, plot_bins_SR, plot_bins_left, plot_bins_right, plot_centers_all, plot_centers_SR, plot_centers_SB = get_bins(SR_left, SR_right, SB_left, SB_right, num_bins_SR = num_bins_SR)\n",
    " \n",
    "    fig, ax = newplot(\"full\", width = 12, height = 9, use_tex = latex_flag)\n",
    "\n",
    "    all_data = np.vstack([data_dict_by_fold[i] for i in range(5)])\n",
    "    all_scores = np.vstack([scores_dict_by_fold[i].reshape(-1,1) for i in range(5)])\n",
    "    all_masses = mass_scalar.inverse_transform(all_data[:,-1].reshape(-1,1))\n",
    "    in_SR = (all_masses >= SR_left ) & (all_masses <= SR_right)\n",
    "    in_SBL = (all_masses < SR_left )\n",
    "    in_SBH = (all_masses > SR_right )\n",
    "\n",
    "    mass_SBL = all_masses[in_SBL]\n",
    "    mass_SR = all_masses[in_SR]\n",
    "    mass_SBH = all_masses[in_SBH]\n",
    "\n",
    "    feature_SBL = all_scores[in_SBL]\n",
    "    feature_SR = all_scores[in_SR]\n",
    "    feature_SBH = all_scores[in_SBH]\n",
    "    \n",
    "    # Get a list of all possible cuts for the feature\n",
    "    feature_cut_points = np.linspace(np.min(all_scores), np.max(all_scores), 10000)\n",
    "       \n",
    "    # For each cut, calculate the number of signal and background events in the SR\n",
    "    num_in_SBL = []\n",
    "    num_in_SR = []\n",
    "    num_in_SBH = []\n",
    "    FPR = []\n",
    "    for cut in feature_cut_points:\n",
    "        num_in_SBL.append(np.sum(feature_SBL >= cut)/len(feature_SBL))\n",
    "        num_in_SR.append(np.sum(feature_SR >= cut)/len(feature_SR))\n",
    "        num_in_SBH.append(np.sum(feature_SBH >= cut)/len(feature_SBH))\n",
    "\n",
    "        FPR.append((np.sum(feature_SBH >= cut)+np.sum(feature_SBL >= cut))/(len(feature_SBH)+len(feature_SBL)))\n",
    "\n",
    "    for t, threshold in enumerate(fpr_thresholds):\n",
    "\n",
    "        # Use interpolation to find the cut point that gives the desired FPR\n",
    "        best_feature_cut = feature_cut_points[np.argmin(np.abs(np.array(FPR)-threshold))]\n",
    "\n",
    "        # Make the cuts to both the feature and the mass\n",
    "        #feature_SBL_cut = feature_SBL[feature_SBL >= best_feature_cut]\n",
    "        #feature_SR_cut = feature_SR[feature_SR >= best_feature_cut]\n",
    "        #feature_SBH_cut = feature_SBH[feature_SBH >= best_feature_cut]\n",
    "\n",
    "        mass_SBL_cut = mass_SBL[feature_SBL >= best_feature_cut]\n",
    "        mass_SR_cut = mass_SR[feature_SR >= best_feature_cut]\n",
    "        mass_SBH_cut = mass_SBH[feature_SBH >= best_feature_cut]\n",
    "\n",
    "        # Concatenate to get the full mass spectrum\n",
    "        filtered_masses = np.concatenate((mass_SBL_cut, mass_SR_cut, mass_SBH_cut))\n",
    "        \n",
    "        # get the fit function to SB background\n",
    "        popt, pcov, chi2, y_vals, n_dof = curve_fit_m_inv(filtered_masses, fit_type, SR_left, SR_right, plot_bins_left, plot_bins_right, plot_centers_SB)\n",
    "        #print(\"chi2/dof:\", chi2/n_dof)\n",
    "        \n",
    "        # plot the fit function\n",
    "        plt.plot(plot_centers_all, fit_function(plot_centers_all, *popt), lw = 2, linestyle = \"dashed\", color = f\"C{t}\")    \n",
    "        function_with_band(ax, fit_function, [SB_left, SB_right], popt, pcov, color = f\"C{t}\")\n",
    "\n",
    "# def function_with_band(ax, f, range, params, pcov = None, color = \"purple\", alpha_line = 0.75, alpha_band = 0.25, lw = 3,  **kwargs):\n",
    "\n",
    "\n",
    "        # calculate significance of bump\n",
    "        S, B, q0 = calculate_test_statistic(filtered_masses, fit_function, fit_type, plot_bins_SR, plot_centers_SR, SR_left, SR_right, popt, pcov)\n",
    "       \n",
    "        significance = np.sqrt(q0)\n",
    "\n",
    "        label_string = str(round(100*threshold, 2))+\" S: \" +str(round(S,0)) + \", B: \" +str(round(B, 0)) + \" Z: \"+str(round(significance,2))\n",
    "\n",
    "        # hist_with_errors(ax, filtered_masses, bins = plot_bins_all, range = (SB_left, SB_right), lw = 3, color = f\"C{t}\",label = label_string)\n",
    "        # hist_with_outline(ax, filtered_masses, bins = plot_bins_all, range = (SB_left, SB_right), lw = 3, color = f\"C{t}\",label = label_string)\n",
    "        plt.hist(filtered_masses, bins = plot_bins_all, lw = 3, histtype = \"step\", color = f\"C{t}\",label = label_string, alpha = 0.75)\n",
    "        plt.scatter(plot_centers_SB, y_vals, color = f\"C{t}\")\n",
    "\n",
    "\n",
    "\n",
    "    legend_title = f\"Upsilon Resonances: Iso, Opp. Sign {feature_id}\"\n",
    "    plt.legend(loc = (0.975, 0.6), fontsize = 16, title = title[:-1])\n",
    "\n",
    "\n",
    "    plt.axvline(SR_left, color= \"k\", lw = 3, zorder = 10)\n",
    "    plt.axvline(SR_right, color= \"k\", lw = 3, zorder = 10)\n",
    "\n",
    "    plt.xlabel(\"$M_{\\mu\\mu}$ [GeV]\", fontsize = 18)\n",
    "    plt.ylabel(\"Events\", fontsize = 18)\n",
    "\n",
    "    #plt.yscale(\"log\")\n",
    "    #plt.ylim(0, 300)\n",
    "\n",
    "    # Add more x ticks (major and minor)\n",
    "    plt.xticks(fontsize = 18)\n",
    "    plt.yticks(fontsize = 18)\n",
    "    plt.minorticks_on()\n",
    "    plt.tick_params(axis='x', which='minor', bottom=True)\n",
    "    plt.tick_params(axis='y', which='minor', left=True)\n",
    "    \n",
    "\n",
    "    # # Vertical Black Lines at boundaries of SR\n",
    "    # plt.axvline(SR_left, color = \"black\", linestyle = \"--\", lw = 2)\n",
    "    # plt.axvline(SR_right, color = \"black\", linestyle = \"--\", lw = 2)\n",
    "\n",
    "\n",
    "    plt.title(title, fontsize = 24)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85723175-0550-42df-84b0-51e84762dd3f",
   "metadata": {},
   "source": [
    "# test:\n",
    "- BDT discriminates `train_data_id` flow samples from `train_data_id` sample i and is evaluated on `train_data_id` sample i\n",
    "- - (i.e. if `train_samesign = False`, then trained on OS flow samples and OS data, evaluated on OS data)\n",
    "- i = 0: OS data, *with signal*\n",
    "- i != 0: bkg-only flow samples from an independent flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae69a3b-9910-4511-a6c1-61884b8b27de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "PLOT HISTOGRAM ON SMALL TEST SET\n",
    "\"\"\"\n",
    "\n",
    "for i in range(num_to_plot):\n",
    "    plot_histograms_with_fits_SB(fpr_thresholds, all_test_data_splits[i], all_scores_splits[i], \n",
    "                              old_scaler, fit_type, num_bins_SR,\n",
    "                              f\"OS samples, bootstrap {i}\\n\", \n",
    "                              SB_left, SR_left, SR_right, SB_right, take_score_avg=False)\n",
    "\n",
    "\n",
    "#fig = plot_features(fpr_thresholds, all_test_data_splits[0], all_scores_splits[0], score_cutoffs[0] , old_scaler, SR_left, SR_right, take_score_avg=False)\n",
    "\n",
    "\"\"\"\n",
    "plot_BDT(all_test_data_splits[0], all_scores_splits[0], score_cutoffs[0], \n",
    "              old_scaler, SR_left, SR_right, take_score_avg=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3253de-32e3-4fd3-be1c-51edcff856b3",
   "metadata": {},
   "source": [
    "# test SS\n",
    "- BDT discriminates `train_data_id` flow samples from `train_data_id` sample i and is evaluated on ` not train_data_id` sample i\n",
    "- - (i.e. if `train_samesign = False`, then trained on OS flow samples and OS data, evaluated on SS data)\n",
    "- i = 0: SS data (no signal)\n",
    "- i != 0: bkg-only flow samples from an independent flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d54a2f-96e3-45b6-8d4c-acb5385032ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "PLOT HISTOGRAM ON ALTERNATIVE TEST SET\n",
    "\"\"\"\n",
    "for i in range(num_to_plot):\n",
    "    plot_histograms_with_fits_SB(fpr_thresholds, all_alt_data_splits[i][\"alt\"], \n",
    "                          all_alt_scores_splits[i][\"alt\"], old_scaler, \n",
    "                          fit_type, num_bins_SR,\n",
    "                          f\"SS samples, bootstrap {i} (trained on flow samples)\\n\",SB_left, \n",
    "                          SR_left, SR_right, SB_right, take_score_avg=False)\n",
    "\n",
    "\n",
    "#plot_features(fpr_thresholds, all_alt_data_splits[pseudo_e_to_plot][\"alt\"], all_alt_scores_splits[pseudo_e_to_plot][\"alt\"], \n",
    "#              score_cutoffs[pseudo_e_to_plot], old_scaler, SR_left, SR_right, take_score_avg=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349e4ec0-06dd-4976-b8d1-b85c961119b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PLOT HISTOGRAM ON FLOW SAMPLES\n",
    "\"\"\"\n",
    "for i in range(num_to_plot):\n",
    "    plot_histograms_with_fits_SB(fpr_thresholds, all_alt_data_splits[i][\"ROC_samples\"], \n",
    "                          all_alt_scores_splits[i][\"ROC_samples\"], old_scaler, \n",
    "                          fit_type,num_bins_SR,\n",
    "                          f\"high stats flow samples (independent set) bootstrap  {i} (trained on flow samples)\\n\", \n",
    "                          SB_left, SR_left, SR_right, SB_right, take_score_avg=False)\n",
    "#plot_features(fpr_thresholds,  all_alt_data_splits[pseudo_e_to_plot][\"ROC_samples\"],all_alt_scores_splits[pseudo_e_to_plot][\"ROC_samples\"], \n",
    "#              score_cutoffs[pseudo_e_to_plot], old_scaler, SR_left, SR_right, take_score_avg=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3da91d-1b52-437e-9e1b-a5d612e81c31",
   "metadata": {},
   "source": [
    "# Plot ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f03938d-7a21-44ef-a26d-fe6f147414e4",
   "metadata": {},
   "source": [
    "Calculate the ROC curves for true S / B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a99712b-a3a2-4567-991c-905debf19f20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# determine fpr thresholds as before\n",
    "# yes this is repeated code\n",
    "fpr_thresholds_finegrained = np.logspace(0, -3, 50)\n",
    "#fpr_thresholds = np.linspace(1, 0 , 50)\n",
    "\n",
    "plot_bins_all, plot_bins_SR, plot_bins_left, plot_bins_right, plot_centers_all, plot_centers_SR, plot_centers_SB = get_bins(SR_left, SR_right, SB_left, SB_right, num_bins_SR = num_bins_SR)\n",
    "\n",
    "\n",
    "# first determine score cutoffs\n",
    "score_cutoffs_finegrained = {pseudo_e:{i:{threshold:0 for threshold in fpr_thresholds_finegrained} for i in range(n_folds)} for pseudo_e in range(num_pseudoexperiments)}\n",
    "\n",
    "for pseudo_e in range(num_pseudoexperiments):\n",
    "    for i_fold in range(n_folds):\n",
    "        loc_scores_sorted = np.sort(1.0-all_alt_scores_splits[pseudo_e][\"FPR_validation\"][i_fold])\n",
    "        for threshold in fpr_thresholds_finegrained:\n",
    "            loc_score_cutoff = 1-loc_scores_sorted[min(int(threshold*len(loc_scores_sorted)),len(loc_scores_sorted)-1)]\n",
    "            score_cutoffs_finegrained[pseudo_e][i_fold][threshold] = loc_score_cutoff\n",
    "\n",
    "        \n",
    "def get_classifier_metrics_high_stats(dataset_by_pseudo_e, scores_by_pseudo_e, score_cutoffs):\n",
    "            \n",
    "    S_yield, B_yield = np.empty((fpr_thresholds_finegrained.shape[0], num_pseudoexperiments)), np.empty((fpr_thresholds_finegrained.shape[0], num_pseudoexperiments))\n",
    "\n",
    "    for pseudo_e in range(num_pseudoexperiments):\n",
    "\n",
    "        print(f\"On pseudo experiment {pseudo_e}...\")\n",
    "        for t, threshold in enumerate(fpr_thresholds_finegrained):\n",
    "\n",
    "            filtered_masses_bs = []\n",
    "\n",
    "            for i_fold in range(n_folds):\n",
    "                loc_true_masses_bs = scaler.inverse_transform(np.array(dataset_by_pseudo_e[pseudo_e][i_fold][:,-1]).reshape(-1,1))\n",
    "                loc_scores_bs = scores_by_pseudo_e[pseudo_e][i_fold]\n",
    "                # filter top event based on score cutoff\n",
    "                loc_filtered_masses_bs, _, _, _ = select_top_events_fold(loc_true_masses_bs, loc_scores_bs, score_cutoffs[pseudo_e][i_fold][threshold], plot_bins_left, plot_bins_right, plot_bins_SR)\n",
    "                filtered_masses_bs.append(loc_filtered_masses_bs)\n",
    "\n",
    "            filtered_masses_bs = np.concatenate(filtered_masses_bs)\n",
    "            # get the fit function to SB background\n",
    "            popt, pcov, chi2, y_vals, n_dof = curve_fit_m_inv(filtered_masses_bs, fit_type, SR_left, SR_right, plot_bins_left, plot_bins_right, plot_centers_SB)\n",
    "            num_S_expected_in_SR, num_B_expected_in_SR = calc_significance(filtered_masses_bs, fit_function, plot_bins_SR, plot_centers_SR, SR_left, SR_right, popt)\n",
    "\n",
    "            y_err = get_errors_bkg_fit_ratio(popt, pcov, plot_centers_SR, fit_type)\n",
    "            B_error = np.sqrt(np.sum(y_err**2))\n",
    "            S_over_B = num_S_expected_in_SR/num_B_expected_in_SR\n",
    "            \n",
    "            \n",
    "            significance = num_S_expected_in_SR/np.sqrt(num_B_expected_in_SR+B_error**2)\n",
    "\n",
    "            # TODO: ERRORS\n",
    "            \n",
    "            S_yield[t, pseudo_e] = num_S_expected_in_SR\n",
    "            B_yield[t, pseudo_e] = num_B_expected_in_SR\n",
    "        \n",
    "    # calculate summary stats\n",
    "    TPR = S_yield/S_yield[0,:]\n",
    "    FPR = B_yield/B_yield[0,:]\n",
    "    \n",
    "    \n",
    "\n",
    "    ROC = 1.0/FPR\n",
    "\n",
    "    SIC = TPR/np.sqrt(FPR)\n",
    "    \n",
    "    return TPR, FPR, ROC, SIC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec045d1c-a6ca-4d89-95aa-c9def41d039a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TPR, FPR, ROC, SIC = get_classifier_metrics_high_stats(all_test_data_splits, all_scores_splits, score_cutoffs_finegrained)\n",
    "\n",
    "TPR_median, TPR_lower, TPR_upper = get_median_percentiles(TPR)\n",
    "FPR_median, FPR_lower, FPR_upper = get_median_percentiles(FPR)\n",
    "ROC_median, ROC_lower, ROC_upper = get_median_percentiles(ROC)\n",
    "SIC_median, SIC_lower, SIC_upper = get_median_percentiles(SIC)\n",
    "\n",
    "all_TPR[(fit_type, num_bins_SR)] = TPR_median, TPR_lower, TPR_upper\n",
    "all_FPR[(fit_type, num_bins_SR)] = FPR_median, FPR_lower, FPR_upper\n",
    "all_ROC[(fit_type, num_bins_SR)] = ROC_median, ROC_lower, ROC_upper\n",
    "all_SIC[(fit_type, num_bins_SR)] = SIC_median, SIC_lower, SIC_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63114e3f-f906-48c8-8926-52865fc030cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "colors_dict = {\"cubic\": \"red\", \"quintic\":\"blue\", \"septic\":\"purple\"}\n",
    "styles_dict = {16:\"solid\", 12:\"dashed\", 8:\"dotted\"}\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for ff, nn in all_TPR.keys():\n",
    "    plt.plot(all_FPR[(ff,nn)][0], all_TPR[(ff,nn)][0], label=f\"{ff}, {nn}\", color = colors_dict[ff], linestyle=styles_dict[nn])\n",
    "    plt.fill_between(all_FPR[(ff,nn)][0], all_TPR[(ff,nn)][1], all_TPR[(ff,nn)][2], alpha = 0.2, color = colors_dict[ff] )\n",
    "    \n",
    "plt.plot(all_FPR[(ff,nn)][0], all_FPR[(ff,nn)][0], linestyle = \"dashed\", color = \"grey\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.legend(loc=(1,0))\n",
    "plt.ylim(-1,6)\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(f\"low-stats data _oppsign (trained on {train_data_id_title})\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cfeaa3-6968-402e-9589-101d2a03abfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = newplot(\"full\", use_tex = latex_flag)\n",
    "for ff, nn in all_TPR.keys():\n",
    "    \n",
    "    ax.plot(all_FPR[(ff,nn)][0], all_SIC[(ff,nn)][0], label=f\"{ff}, {nn}\", color = colors_dict[ff], linestyle=styles_dict[nn])\n",
    "    ax.fill_between(all_FPR[(ff,nn)][0], all_SIC[(ff,nn)][1], all_SIC[(ff,nn)][2], alpha = 0.2, color = colors_dict[ff] )\n",
    "ax.plot(all_FPR[(ff,nn)][0], all_FPR[(ff,nn)][0]/np.sqrt(all_FPR[(ff,nn)][0]), linestyle = \"dashed\", color = \"grey\")\n",
    "plt.legend(loc=(1,0))\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"FPR\")\n",
    "ax.set_ylim(-1,30)\n",
    "ax.set_ylabel(\"$S/\\sqrt{B}$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c078a-1654-4aee-9cde-f0d20fa8aaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53d9e124-fe68-4fe2-8051-d17e8d4ec98c",
   "metadata": {},
   "source": [
    "Calculate the ROC curves for data vs cathode samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a70765-7c75-4a97-bfa0-89e2aa0b6d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_interp = np.linspace(1e-5, 1, 100)\n",
    "TPR = np.zeros((fpr_interp.shape[0], num_pseudoexperiments))\n",
    "\n",
    "for pseudo_e in range(num_pseudoexperiments):\n",
    "\n",
    "    print(f\"On pseudo experiment {pseudo_e}...\")\n",
    "    \n",
    "    scores_pseudo_e_data = []\n",
    "    scores_pseudo_e_samples = []\n",
    "\n",
    "    for i_fold in range(n_folds):\n",
    "        scores_pseudo_e_data.append(all_scores_splits[pseudo_e][i_fold])\n",
    "        scores_pseudo_e_samples.append(all_alt_scores_splits[pseudo_e][\"FPR_validation\"][i_fold])\n",
    "\n",
    "    scores_pseudo_e_data = np.concatenate(scores_pseudo_e_data)\n",
    "    scores_pseudo_e_samples = np.concatenate(scores_pseudo_e_samples)\n",
    "\n",
    "    scores_all = np.hstack([scores_pseudo_e_data,scores_pseudo_e_samples])\n",
    "    labels_all = np.hstack([np.ones((scores_pseudo_e_data.shape[0],)),np.zeros((scores_pseudo_e_samples.shape[0],))])\n",
    "    \n",
    "    loc_fpr, loc_tpr, _ = roc_curve(labels_all, scores_all)\n",
    "    tpr_interp = np.interp(fpr_interp, loc_fpr, loc_tpr)\n",
    "    \n",
    "    TPR[:,pseudo_e] = tpr_interp\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63527dd-435e-4deb-890e-450ed0ee2625",
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR_median, TPR_lower, TPR_upper = get_median_percentiles(TPR)\n",
    "\n",
    "plt.plot(fpr_interp,TPR_median, lw = 0.5)\n",
    "plt.fill_between(fpr_interp, TPR_lower, TPR_upper, alpha = 0.2)\n",
    "\n",
    "plt.plot(fpr_interp,fpr_interp, linestyle = \"dashed\", color = \"grey\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b253df6-2758-49dc-8719-32e6b1c927c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bedd38-9e0b-4d53-b50e-3baa07b6c075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef413bc7-48f6-4808-a212-8e04e714af5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc5ab5-8c4d-475f-82ef-5762b98f8a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d47a3-053f-48e6-b4e2-6c005e71d104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
